{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned ON\n"
     ]
    }
   ],
   "source": [
    "%pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import functools\n",
    "import statistics\n",
    "import itertools\n",
    "import random\n",
    "import math\n",
    "from pathlib import Path\n",
    "import pdb\n",
    "\n",
    "import pandas as pd\n",
    "import swifter\n",
    "import numpy as np\n",
    "import hickle as hkl\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import spacy\n",
    "from cached_property import cached_property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'nlp' not in vars():\n",
    "    nlp = spacy.load(\n",
    "        \"en_core_web_lg\",\n",
    "        disable=[\"tagger\", \"ner\", \"textcat\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'articles' not in vars():\n",
    "    articles = pd.read_parquet(\"data/articles-processed.parquet.gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNModel(nn.Module):\n",
    "    def __init__(self, *_args, **_kwargs):\n",
    "        super().__init__()\n",
    "        \n",
    "        self._args = _args\n",
    "        self._kwargs = _kwargs\n",
    "        \n",
    "    def save(self, path):\n",
    "        torch.save(\n",
    "            {\n",
    "                'state': self.state_dict(),\n",
    "                'args': self._args,\n",
    "                'kwargs': self._kwargs\n",
    "            },\n",
    "            path\n",
    "        )\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, path):\n",
    "        if Path(path).exists():\n",
    "            data = torch.load(path)\n",
    "            \n",
    "            model = cls(*data['args'], **data['kwargs'])\n",
    "            model.load_state_dict(data['state'])\n",
    "\n",
    "            return model\n",
    "        else:\n",
    "            raise FileNotFoundError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscriminatorNet(NNModel):\n",
    "    def __init__(self, input_size):\n",
    "        super(DiscriminatorNet, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        \n",
    "        self.linear = nn.Linear(input_size, 1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"\n",
    "        The forward pass for the network\n",
    "        \n",
    "        hidden_state : tensor (batch_num, hidden_size)\n",
    "        \n",
    "        returns         : tensor (batch_num, 1)\n",
    "        \"\"\"\n",
    "        \n",
    "        state = state.transpose(0, 1).reshape(-1, self.input_size)\n",
    "        state = self.linear(state)\n",
    "        state = F.sigmoid(state)\n",
    "        \n",
    "        return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SummarizeNet(NNModel):\n",
    "    def __init__(self, hidden_size, input_size, num_layers, vocabulary_size, cutoffs):\n",
    "        super(SummarizeNet, self).__init__(\n",
    "            hidden_size=hidden_size,\n",
    "            input_size=input_size,\n",
    "            num_layers=num_layers,\n",
    "            vocabulary_size=vocabulary_size\n",
    "        )\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.encode_gru = nn.GRU(\n",
    "            input_size,\n",
    "            hidden_size,\n",
    "            num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        \n",
    "        self.decode_gru = nn.GRU(\n",
    "            hidden_size,\n",
    "            input_size,\n",
    "            num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        \n",
    "        self.discriminate = DiscriminatorNet(num_layers * 2 * input_size)\n",
    "        \n",
    "        self.adaptive_softmax = nn.AdaptiveLogSoftmaxWithLoss(\n",
    "            in_features=input_size,\n",
    "            n_classes=vocabulary_size,\n",
    "            cutoffs=cutoffs\n",
    "        )\n",
    "        \n",
    "    def take_last_pass(self, predicted):\n",
    "        return predicted.reshape(\n",
    "            predicted.shape[0],\n",
    "            predicted.shape[1],\n",
    "            2,\n",
    "            int(predicted.shape[2] / 2)\n",
    "        )[:, :, 1, :]\n",
    "\n",
    "    def forward(self, word_embeddings, modes, target_probs):\n",
    "        \"\"\"\n",
    "        The forward pass for the network\n",
    "        \n",
    "        word_embeddings : tensor (batch_num, max_seq_len, embedding_length)\n",
    "        \n",
    "        returns         : tuple (\n",
    "                            tensor (batch_num, max_seq_len, vocab_len),\n",
    "                            tensor (batch_num, hidden_size)\n",
    "                          )\n",
    "        \n",
    "        First tensor in the returning tuple are predicted word embeddings\n",
    "        The second tensor are probabilities of the output being a headline\n",
    "        \"\"\"\n",
    "        \n",
    "        predicted, _ = self.encode_gru(word_embeddings)\n",
    "        predicted = self.take_last_pass(predicted)\n",
    "        \n",
    "        predicted, state = self.decode_gru(predicted)\n",
    "        predicted = self.take_last_pass(predicted)\n",
    "        \n",
    "        # pdb.set_trace()\n",
    "        predicted_probs, loss = self.adaptive_softmax(predicted, target_probs)\n",
    "        \n",
    "        predicted_modes = self.discriminate(state)\n",
    "        \n",
    "        return predicted_probs, predicted_modes, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArticlesDataset(Dataset):\n",
    "    def __init__(self, dataframe, mode, transforms=[]):\n",
    "        if mode not in ['train', 'test', 'val']:\n",
    "            raise ValueError(f\"{mode} not in the set of modes of the dataset (['train', 'test', 'val'])\")\n",
    "            \n",
    "        self.data = dataframe[dataframe.set == mode]\n",
    "        self.transforms = transforms\n",
    "        self.mode = mode\n",
    "        \n",
    "    def __len__(self):\n",
    "        return 2*len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        _idx = []\n",
    "        \n",
    "        if torch.is_tensor(idx):\n",
    "            _idx = idx.tolist()\n",
    "        \n",
    "        if isinstance(idx, list):\n",
    "            _idx = idx\n",
    "        else:\n",
    "            _idx = [ idx ]\n",
    "        \n",
    "        _ids = [ (i - (i % 2))/2 for i in _idx]\n",
    "\n",
    "        data = self.data.iloc[_ids, :]\n",
    "        data['asked_id'] = _idx\n",
    "        \n",
    "        data = pd.DataFrame(\n",
    "            {\n",
    "                'set': [self.mode for _ in range(0, len(_ids))],\n",
    "                'mode': np.array([ (0.0 if i % 2 == 0 else 1.0) for i in _idx ]),\n",
    "                'text': data.apply(lambda row: row['text'] if row['asked_id'] % 2 == 0 else row['headline'], axis=1),\n",
    "                'title': data['normalized_title']\n",
    "            }\n",
    "        )\n",
    "\n",
    "        for transform in self.transforms:\n",
    "            data = transform(data)\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextToParsedDoc(object):\n",
    "    def __init__(self, nlp):\n",
    "        self.nlp = nlp\n",
    "        \n",
    "    def __call__(self, sample):\n",
    "        sample['doc'] = sample.swifter.progress_bar(False).apply(lambda row: self.nlp(row['text']), axis=1)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordsToVectors(object):\n",
    "    def __init__(self, nlp):\n",
    "        self.nlp = nlp\n",
    "        \n",
    "    def document_embeddings(self, doc):\n",
    "        word_embeddings = [\n",
    "            [ l.vector ] if l.whitespace_ == '' else [ l.vector, np.zeros_like(l.vector) ] for l in doc\n",
    "        ]\n",
    "\n",
    "        return np.stack(\n",
    "            [\n",
    "                vector for vectors in word_embeddings for vector in vectors\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        \n",
    "        sample['word_embeddings'] = sample.swifter.progress_bar(False).apply(\n",
    "            lambda row: self.document_embeddings(row['doc']),\n",
    "            axis=1\n",
    "        )\n",
    "    \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddNoiseToEmbeddings(object):\n",
    "    def __init__(self, probability_of_mask_for_word):\n",
    "        self.probability_of_mask_for_word = probability_of_mask_for_word\n",
    "        self.rng = np.random.default_rng()\n",
    "        \n",
    "    def mask_vector(self, vector):\n",
    "        \"\"\"\n",
    "        Masks words with zeros randomly\n",
    "        \"\"\"\n",
    "        seq_len = vector.shape[0]\n",
    "        vector_len = vector.shape[1]\n",
    "        \n",
    "        mask = np.repeat(\n",
    "            self.rng.choice(\n",
    "                [0, 1],\n",
    "                seq_len,\n",
    "                p=[\n",
    "                    self.probability_of_mask_for_word,\n",
    "                    (1 - self.probability_of_mask_for_word)\n",
    "                ]\n",
    "            ).reshape((seq_len, 1)),\n",
    "            vector_len,\n",
    "            axis=1\n",
    "        )\n",
    "        \n",
    "        return vector * mask\n",
    "        \n",
    "    def __call__(self, sample):       \n",
    "        sample['noisy_word_embeddings'] = sample['word_embeddings'].apply(self.mask_vector)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MergeBatch(object):\n",
    "    def __init__(self, device):\n",
    "        self.device = device\n",
    "        \n",
    "    def stack_vectors(self, vectors):\n",
    "        max_seq = max([vector.shape[0] for vector in vectors])\n",
    "        \n",
    "        return np.stack(\n",
    "            [\n",
    "                np.pad(vector, [(0, max_seq - vector.shape[0]), (0, 0)])\n",
    "                for vector in vectors\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "    def __call__(self, sample):\n",
    "        del sample['doc']\n",
    "        \n",
    "        sample = sample.to_dict(orient=\"list\")\n",
    "        \n",
    "        sample['word_embeddings'] = torch.from_numpy(\n",
    "            self.stack_vectors(\n",
    "                sample['word_embeddings']\n",
    "            ).astype(np.float32, copy=False)\n",
    "        ).to(self.device)\n",
    "        \n",
    "        if 'noisy_word_embeddings' in sample:\n",
    "            sample['noisy_word_embeddings'] = torch.from_numpy(\n",
    "                self.stack_vectors(\n",
    "                    sample['noisy_word_embeddings']\n",
    "                ).astype(np.float32, copy=False)\n",
    "            ).to(self.device)\n",
    "        \n",
    "        sample['mode'] = torch.from_numpy(\n",
    "            np.stack(\n",
    "                sample['mode']\n",
    "            ).astype(np.float32, copy=False)\n",
    "        ).to(self.device)\n",
    "    \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SetAllToSummarizing(object):\n",
    "    def __call__(self, sample):\n",
    "        sample['mode'] = np.ones_like(sample['mode']).astype(np.float32, copy=False)\n",
    "        \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    def __init__(self, nlp, series):\n",
    "        self.nlp = nlp\n",
    "        \n",
    "        if Path(\"vocabulary.pickle\").exists():\n",
    "            with open('vocabulary.pickle', 'rb') as handle:\n",
    "                data = pickle.load(handle)\n",
    "            \n",
    "            self.words = data['words']\n",
    "            self.index = data['index']\n",
    "        else:\n",
    "            text = \"\"\n",
    "            words = []\n",
    "            index = {}\n",
    "            counts = {}\n",
    "            \n",
    "            for serie in series:\n",
    "                for text in serie.fillna('').values.tolist():\n",
    "                    text_counts = nlp(text).count_by(spacy.attrs.LOWER)\n",
    "                    \n",
    "                    for ix in text_counts:\n",
    "                        if ix in counts:\n",
    "                            counts[ix] += text_counts[ix]\n",
    "                        else:\n",
    "                            counts[ix] = text_counts[ix]\n",
    "\n",
    "            for ix, _ in sorted([(ix, counts[ix]) for ix in counts],key=lambda t: t[1],reverse=True):\n",
    "                words.append(nlp.vocab[ix].text)\n",
    "                index[ix] = len(words)\n",
    "                \n",
    "            self.words = words\n",
    "            self.index = index\n",
    "            \n",
    "            with open('vocabulary.pickle', 'wb') as handle:\n",
    "                pickle.dump({'words': self.words, 'index': self.index}, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.words)\n",
    "    \n",
    "    def decode(self, probs):\n",
    "        \"\"\"\n",
    "        probs: BxSxV tensor where:\n",
    "          B = batch size\n",
    "          S = sequence length\n",
    "          V = vocabulary size\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader(object):\n",
    "    def __init__(self, dataset, batch_size=8):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    @property\n",
    "    def epoch_size(self):\n",
    "        return math.ceil(len(self.dataset) / self.batch_size) * self.batch_size\n",
    "    \n",
    "    def __iter__(self):\n",
    "        ids = random.choices(range(0, len(self.dataset)), k=self.epoch_size)\n",
    "        \n",
    "        for start_ix in range(0, self.epoch_size, self.batch_size):\n",
    "            yield self.dataset[ids[start_ix:(start_ix + self.batch_size)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArticlesBatch:\n",
    "    def __init__(self, data, ix=0):\n",
    "        self.data = data\n",
    "        self.ix = ix\n",
    "    \n",
    "        \n",
    "    def __getattr__(self, name):\n",
    "        if name in self.data:\n",
    "            return self.data[name]\n",
    "        else:\n",
    "            raise AttributeError(f\"Attribute missing: {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(object):\n",
    "    def __init__(self, vocabulary):\n",
    "        self.vocabulary = vocabulary\n",
    "\n",
    "    def decode_embeddings(self, word_embeddings):\n",
    "        pass\n",
    "#         data = word_embeddings.cpu().data.numpy()\n",
    "        \n",
    "#         return [\n",
    "#             self.decode_embeddings_1d(data[ix, :, :])\n",
    "#             for ix in range(0, data.shape[0])\n",
    "#         ]\n",
    "        \n",
    "    def decode_embeddings_1d(self, word_embeddings):\n",
    "        \"\"\"\n",
    "        Decodes a single document. Word embeddings given are of shape (N, D)\n",
    "        where N is the number of lexemes and D the dimentionality of the embedding vector\n",
    "        \"\"\"\n",
    "        \n",
    "        pass\n",
    "        \n",
    "#         return \"\".join(\n",
    "#             [\n",
    "#                 token.text.lower() if not token.is_oov else \" \"\n",
    "#                 for token in [\n",
    "#                     self.nlp.vocab[ks[0]]\n",
    "#                     for ks in self.nlp.vocab.vectors.most_similar(\n",
    "#                         word_embeddings, n=1\n",
    "#                     )[0]\n",
    "#                 ]\n",
    "#             ]\n",
    "#         ).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metrics(object):\n",
    "    def __init__(self, mode, loss=None):\n",
    "        self.mode = mode\n",
    "        self.losses = [loss.cpu().item()] if loss is not None else []\n",
    "    \n",
    "    @classmethod\n",
    "    def empty(cls, mode):\n",
    "        return cls(mode)\n",
    "    \n",
    "    @property\n",
    "    def loss(self):\n",
    "        if len(self.losses) == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return statistics.mean(self.losses)\n",
    "    \n",
    "    @property\n",
    "    def last_loss(self):\n",
    "        return self.losses[len(self.losses) - 1]\n",
    "    \n",
    "    def running_mean_loss(self, n=100):\n",
    "        cumsum = np.cumsum(np.insert(np.array(self.losses), 0, 0)) \n",
    "        return (cumsum[n:] - cumsum[:-n]) / float(n)\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        self.losses += other.losses\n",
    "        \n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpdateInfo(object):\n",
    "    def __init__(self, decoder, batch, word_embeddings, loss_sum, mode):\n",
    "        self.decoder = decoder\n",
    "        self.batch = batch\n",
    "        self.word_embeddings = word_embeddings\n",
    "        self.loss_sum = loss_sum\n",
    "        self.mode = mode\n",
    "        \n",
    "    @property\n",
    "    def from_train(self):\n",
    "        return self.mode == \"train\"\n",
    "    \n",
    "    @property\n",
    "    def from_evaluate(self):\n",
    "        return self.mode == \"val\"\n",
    "        \n",
    "    @cached_property\n",
    "    def decoded_inferred_texts(self):\n",
    "        return self.decoder.decode_embeddings(self.word_embeddings)\n",
    "    \n",
    "    @cached_property\n",
    "    def metrics(self):\n",
    "        return Metrics(self.mode, self.loss_sum)\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"{self.mode} | {self.batch.ix}\\t| Loss: {loss_sum}\\t\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseTrainer:\n",
    "    def __init__(self, name, vocabulary, dataframe,\n",
    "                 optimizer_class_name,\n",
    "                 model_args, optimizer_args, \n",
    "                 batch_size, update_every,\n",
    "                 probability_of_mask_for_word,\n",
    "                 device\n",
    "                ):\n",
    "        self.name = name\n",
    "        \n",
    "        self.device = device\n",
    "        \n",
    "        self.datasets = {\n",
    "            \"train\": ArticlesDataset(\n",
    "                dataframe,\n",
    "                \"train\",\n",
    "                transforms=[\n",
    "                    TextToParsedDoc(vocabulary.nlp),\n",
    "                    WordsToVectors(vocabulary.nlp),\n",
    "                    AddNoiseToEmbeddings(probability_of_mask_for_word),\n",
    "                    MergeBatch(device)\n",
    "                ]\n",
    "            ),\n",
    "            \"test\":  ArticlesDataset(\n",
    "                dataframe,\n",
    "                \"test\",\n",
    "                transforms=[\n",
    "                    TextToParsedDoc(vocabulary.nlp),\n",
    "                    WordsToVectors(vocabulary.nlp),\n",
    "                    AddNoiseToEmbeddings(0),\n",
    "                    SetAllToSummarizing(),\n",
    "                    MergeBatch(device)\n",
    "                ]\n",
    "            ),\n",
    "            \"val\":  ArticlesDataset(\n",
    "                dataframe,\n",
    "                \"val\",\n",
    "                transforms=[\n",
    "                    TextToParsedDoc(vocabulary.nlp),\n",
    "                    WordsToVectors(vocabulary.nlp),\n",
    "                    AddNoiseToEmbeddings(0),\n",
    "                    MergeBatch(device)\n",
    "                ]\n",
    "            )\n",
    "        }\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.update_every = update_every\n",
    "        \n",
    "        self.optimizer_class_name = optimizer_class_name\n",
    "        \n",
    "        self.model_args = model_args\n",
    "        self.optimizer_args = optimizer_args\n",
    "        \n",
    "        self.current_batch_id = 0\n",
    "        \n",
    "        self.decoder = Decoder(vocabulary)\n",
    "        \n",
    "        if self.has_checkpoint:\n",
    "            self.load_last_checkpoint()\n",
    "        \n",
    "    @cached_property\n",
    "    def model(self):\n",
    "        try:\n",
    "            return SummarizeNet.load(f\"{self.checkpoint_path}/model.pth\").to(self.device)\n",
    "        except FileNotFoundError:\n",
    "            return SummarizeNet(**self.model_args).to(self.device)\n",
    "        \n",
    "    @cached_property\n",
    "    def optimizer(self):\n",
    "        class_ = getattr(torch.optim, self.optimizer_class_name)\n",
    "        \n",
    "        return class_(self.model.parameters(), **self.optimizer_args)\n",
    "    \n",
    "    @property\n",
    "    def checkpoint_path(self):\n",
    "        return f\"checkpoints/{self.name}/batch-#{self.current_batch_id}\"\n",
    "    \n",
    "    def save_checkpoint(self):\n",
    "        os.makedirs(self.checkpoint_path, exist_ok=True)\n",
    "        \n",
    "        self.model.save(f\"{self.checkpoint_path}/model.pth\")\n",
    "        \n",
    "        torch.save(\n",
    "            {\n",
    "                'current_batch_id': self.current_batch_id,\n",
    "                'batch_size': self.batch_size,\n",
    "                'update_every': self.update_every,\n",
    "                'optimizer_class_name': self.optimizer_class_name,\n",
    "                'optimizer_args': self.optimizer_args,\n",
    "                'optimizer_state_dict': self.optimizer.state_dict()\n",
    "            },\n",
    "            f\"{self.checkpoint_path}/trainer.pth\"\n",
    "        )\n",
    "    \n",
    "    @property\n",
    "    def checkpoint_directories(self):\n",
    "        return sorted(Path(\".\").glob(f\"checkpoints/{self.name}/batch-*\"), reverse=True)\n",
    "    \n",
    "    @property\n",
    "    def has_checkpoint(self):\n",
    "        return len(self.checkpoint_directories) > 0\n",
    "    \n",
    "    def load_last_checkpoint(self):\n",
    "        path = self.checkpoint_directories[0]\n",
    "        \n",
    "        data = torch.load(f\"{path}/trainer.pth\")\n",
    "        \n",
    "        self.batch_size = data['batch_size']\n",
    "        self.update_every = data['update_every']\n",
    "        \n",
    "        self.optimizer_class_name = data['optimizer_class_name']\n",
    "        self.optimizer_args = data['optimizer_args']\n",
    "        \n",
    "        self.current_batch_id = data['current_batch_id']\n",
    "        \n",
    "        if 'model' in self.__dict__:\n",
    "            del self.__dict__['model']\n",
    "            \n",
    "        if 'optimzer' in self.__dict__:\n",
    "            del self.__dict__['optimizer']\n",
    "        \n",
    "        self.optimizer.load_state_dict(data['optimizer_state_dict'])\n",
    "    \n",
    "    def batches(self, mode):       \n",
    "        while True:\n",
    "            loader = DataLoader(\n",
    "                self.datasets[mode],\n",
    "                batch_size=self.batch_size\n",
    "            )\n",
    "\n",
    "            for data in loader:\n",
    "                self.current_batch_id += 1\n",
    "                \n",
    "                yield(\n",
    "                    ArticlesBatch(\n",
    "                        data,\n",
    "                        ix=self.current_batch_id\n",
    "                    )\n",
    "                )\n",
    "    \n",
    "    def work_batch(self, batch):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def updates(self, mode=\"train\", update_every=None):\n",
    "        batches = self.batches(mode)\n",
    "        loss_sum = 0\n",
    "        \n",
    "        if update_every is None:\n",
    "            update_every = self.update_every\n",
    "        \n",
    "        for batch in batches:\n",
    "            if mode == \"train\":\n",
    "                self.model.train()\n",
    "            else:\n",
    "                self.model.eval()\n",
    "            \n",
    "            loss, word_embeddings = self.work_batch(batch)\n",
    "            loss /= self.update_every\n",
    "            \n",
    "            if mode == \"train\":\n",
    "                loss.backward()\n",
    "                \n",
    "            loss_sum += loss\n",
    "            \n",
    "            # we're doing the accumulated gradients trick to get the gradients variance\n",
    "            # down while being able to use commodity GPU:\n",
    "            if batch.ix % update_every == 0:\n",
    "                if mode == \"train\":\n",
    "                    self.optimizer.step()\n",
    "                    self.optimizer.zero_grad()\n",
    "                \n",
    "                yield(UpdateInfo(self.decoder, batch, word_embeddings, loss_sum, mode=mode))\n",
    "                \n",
    "                loss_sum = 0\n",
    "    \n",
    "    def train_and_evaluate_updates(self, evaluate_every=100):\n",
    "        train_updates = self.updates(mode=\"train\")\n",
    "        evaluate_updates = self.updates(mode=\"val\")\n",
    "        \n",
    "        for update_info in train_updates:\n",
    "            yield(update_info)\n",
    "            \n",
    "            if update_info.batch.ix != 0 and update_info.batch.ix % evaluate_every == 0:\n",
    "                yield(next(evaluate_updates))\n",
    "    \n",
    "    def test_updates(self):\n",
    "        return self.updates(mode=\"test\", update_every=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(BaseTrainer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(Trainer, self).__init__(*args, **kwargs)\n",
    "        \n",
    "    def compute_loss(self, word_embeddings, original_word_embeddings, discriminate_probs): \n",
    "        embeddings_loss = F.cosine_embedding_loss(\n",
    "          word_embeddings.reshape((-1, word_embeddings.shape[2])),\n",
    "          original_word_embeddings.reshape((-1, original_word_embeddings.shape[2])),\n",
    "          torch.ones(word_embeddings.shape[0] * word_embeddings.shape[1]).to(self.device)\n",
    "        )\n",
    "        \n",
    "        discriminator_loss = F.binary_cross_entropy(\n",
    "            discriminate_probs,\n",
    "            torch.zeros_like(discriminate_probs).to(self.device)\n",
    "        )\n",
    "        \n",
    "        return embeddings_loss + discriminator_loss\n",
    "        \n",
    "\n",
    "    def work_batch(self, batch):\n",
    "        word_embeddings, discriminate_probs = self.model(\n",
    "            batch.noisy_word_embeddings,\n",
    "            batch.mode\n",
    "        )\n",
    "\n",
    "        # we're diverging from the article here by outputting the word embeddings\n",
    "        # instead of the probabilities for each word in a vocabulary\n",
    "        # our loss function is using the cosine embedding loss coupled with\n",
    "        # the discriminator loss:\n",
    "        return (\n",
    "            self.compute_loss(word_embeddings, batch.word_embeddings, discriminate_probs),\n",
    "            word_embeddings\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InNotebookTrainer(Trainer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(InNotebookTrainer, self).__init__(*args, **kwargs)\n",
    "        \n",
    "        self.writer = SummaryWriter(comment=self.name)\n",
    "\n",
    "    def train(self, evaluate_every=1000):\n",
    "        test_updates = self.test_updates()\n",
    "        \n",
    "        cumulative_train_metrics = Metrics.empty(mode=\"train\")\n",
    "        cumulative_evaluate_metrics = Metrics.empty(mode=\"eval\")\n",
    "\n",
    "        for update_info in self.train_and_evaluate_updates(evaluate_every=evaluate_every):\n",
    "            if update_info.from_train:\n",
    "                cumulative_train_metrics += update_info.metrics\n",
    "                \n",
    "                print(f\"{update_info.batch.ix}\")\n",
    "                \n",
    "                self.writer.add_scalar(\n",
    "                    'loss/train',\n",
    "                    update_info.metrics.loss,\n",
    "                    update_info.batch.ix\n",
    "                )\n",
    "\n",
    "            if update_info.from_evaluate:\n",
    "                cumulative_evaluate_metrics += update_info.metrics\n",
    "                \n",
    "                self.writer.add_scalar(\n",
    "                    'loss/eval',\n",
    "                    update_info.metrics.loss,\n",
    "                    update_info.batch.ix\n",
    "                )\n",
    "\n",
    "                print(f\"Eval: {update_info.metrics.loss}\")\n",
    "                print(f\"Saving checkpoint\")\n",
    "                self.save_checkpoint()\n",
    "\n",
    "#             if update_info.batch.ix % 1000 == 0 and update_info.batch.ix != 0:\n",
    "#                 test_update = next(test_updates)\n",
    "                \n",
    "#                 self.test_texts_stream.write(\n",
    "#                     (\n",
    "#                         update_info.batch.text,\n",
    "#                         update_info.decoded_inferred_texts\n",
    "#                     )\n",
    "#                 )\n",
    "                \n",
    "    def test(self):\n",
    "        cumulative_metrics = Metrics.empty(mode=\"test\")\n",
    "        \n",
    "        for update_info in self.test_updates():\n",
    "            cumulative_metrics += update_info.metrics\n",
    "\n",
    "        print(cumulative_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_TESTS = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "from hypothesis import given, settings, note, assume, reproduce_failure\n",
    "import hypothesis.strategies as st\n",
    "import hypothesis.extra.numpy as npst\n",
    "\n",
    "class TestNotebook(unittest.TestCase):\n",
    "    def test_trainer_batches_yields_proper_ixs(self):\n",
    "        vocabulary = Vocabulary(nlp, [ articles[\"text\"], articles[\"headline\"] ])\n",
    "        \n",
    "        for mode in ['train', 'test', 'val']:\n",
    "            trainer = Trainer(\n",
    "                'unit-test-run-1',\n",
    "                vocabulary,\n",
    "                articles,\n",
    "                optimizer_class_name='Adam',\n",
    "                model_args={\n",
    "                    'hidden_size': 128,\n",
    "                    'input_size': 300,\n",
    "                    'num_layers': 2,\n",
    "                    'cutoffs': [1, 2],\n",
    "                    'vocabulary_size': len(vocabulary)\n",
    "                },\n",
    "                optimizer_args={},\n",
    "                batch_size=32,\n",
    "                update_every=1,\n",
    "                probability_of_mask_for_word=0.3,\n",
    "                device=torch.device('cpu')\n",
    "            )\n",
    "            self.assertGreater(len(trainer.datasets[mode]), 0)\n",
    "            ixs = [batch.ix for batch in itertools.islice(trainer.batches(mode), 10)]\n",
    "            self.assertEqual(list(ixs), list(range(1, 11)))\n",
    "            \n",
    "    @given(\n",
    "        st.sampled_from([4, 8, 12]),\n",
    "        st.sampled_from([100, 200]),\n",
    "        st.sampled_from([32, 64, 128]),\n",
    "        st.sampled_from([1, 2, 3]),\n",
    "        st.sampled_from([100, 200])\n",
    "    )\n",
    "    @settings(max_examples=10)\n",
    "    def test_summarize_net_returns_correct_shapes(self, batch_size, seq_len, hidden_size, num_layers, vocabulary_size):\n",
    "        model = SummarizeNet(\n",
    "            hidden_size=hidden_size,\n",
    "            input_size=300,\n",
    "            num_layers=num_layers,\n",
    "            vocabulary_size=vocabulary_size,\n",
    "            cutoffs=[1, vocabulary_size - 1]\n",
    "        )\n",
    "        \n",
    "        embeddings = torch.rand((batch_size, seq_len, 300))\n",
    "        target = torch.rand((batch_size, seq_len)).int()\n",
    "        modes = torch.rand((batch_size))\n",
    "        \n",
    "        pred_probs, pred_modes, loss = model(embeddings, target, modes)\n",
    "        \n",
    "        self.assertEqual(pred_probs.shape[0], batch_size)\n",
    "        self.assertEqual(pred_probs.shape[1], seq_len)\n",
    "        self.assertEqual(len(pred_probs.shape), 2)\n",
    "        \n",
    "        self.assertEqual(pred_modes.shape[0], batch_size)\n",
    "        self.assertEqual(len(pred_modes.shape), 1)\n",
    "        \n",
    "        self.assertGreater(loss.item(), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> <ipython-input-474-1981a59aaafd>(66)forward()\n",
      "-> predicted_probs, loss = self.adaptive_softmax(predicted, target_probs)\n",
      "(Pdb) help\n",
      "\n",
      "Documented commands (type help <topic>):\n",
      "========================================\n",
      "EOF    c          d        h         list      q        rv       undisplay\n",
      "a      cl         debug    help      ll        quit     s        unt      \n",
      "alias  clear      disable  ignore    longlist  r        source   until    \n",
      "args   commands   display  interact  n         restart  step     up       \n",
      "b      condition  down     j         next      return   tbreak   w        \n",
      "break  cont       enable   jump      p         retval   u        whatis   \n",
      "bt     continue   exit     l         pp        run      unalias  where    \n",
      "\n",
      "Miscellaneous help topics:\n",
      "==========================\n",
      "exec  pdb\n",
      "\n",
      "(Pdb) step\n",
      "--Call--\n",
      "> /usr/local/lib64/python3.6/site-packages/torch/nn/modules/module.py(562)__getattr__()\n",
      "-> def __getattr__(self, name):\n",
      "(Pdb) step\n",
      "> /usr/local/lib64/python3.6/site-packages/torch/nn/modules/module.py(563)__getattr__()\n",
      "-> if '_parameters' in self.__dict__:\n",
      "(Pdb) s\n",
      "> /usr/local/lib64/python3.6/site-packages/torch/nn/modules/module.py(564)__getattr__()\n",
      "-> _parameters = self.__dict__['_parameters']\n",
      "(Pdb) s\n",
      "> /usr/local/lib64/python3.6/site-packages/torch/nn/modules/module.py(565)__getattr__()\n",
      "-> if name in _parameters:\n",
      "(Pdb) s\n",
      "> /usr/local/lib64/python3.6/site-packages/torch/nn/modules/module.py(567)__getattr__()\n",
      "-> if '_buffers' in self.__dict__:\n",
      "(Pdb) s\n",
      "> /usr/local/lib64/python3.6/site-packages/torch/nn/modules/module.py(568)__getattr__()\n",
      "-> _buffers = self.__dict__['_buffers']\n",
      "(Pdb) s\n",
      "> /usr/local/lib64/python3.6/site-packages/torch/nn/modules/module.py(569)__getattr__()\n",
      "-> if name in _buffers:\n",
      "(Pdb) s\n",
      "> /usr/local/lib64/python3.6/site-packages/torch/nn/modules/module.py(571)__getattr__()\n",
      "-> if '_modules' in self.__dict__:\n",
      "(Pdb) s\n",
      "> /usr/local/lib64/python3.6/site-packages/torch/nn/modules/module.py(572)__getattr__()\n",
      "-> modules = self.__dict__['_modules']\n",
      "(Pdb) s\n",
      "> /usr/local/lib64/python3.6/site-packages/torch/nn/modules/module.py(573)__getattr__()\n",
      "-> if name in modules:\n",
      "(Pdb) s\n",
      "> /usr/local/lib64/python3.6/site-packages/torch/nn/modules/module.py(574)__getattr__()\n",
      "-> return modules[name]\n",
      "(Pdb) s\n",
      "--Return--\n",
      "> /usr/local/lib64/python3.6/site-packages/torch/nn/modules/module.py(574)__getattr__()->AdaptiveLogSo...e)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "-> return modules[name]\n",
      "(Pdb) s\n",
      "--Call--\n",
      "> /usr/local/lib64/python3.6/site-packages/torch/nn/modules/module.py(522)__call__()\n",
      "-> def __call__(self, *input, **kwargs):\n",
      "(Pdb) s\n",
      "> /usr/local/lib64/python3.6/site-packages/torch/nn/modules/module.py(523)__call__()\n",
      "-> for hook in self._forward_pre_hooks.values():\n",
      "(Pdb) s\n",
      "> /usr/local/lib64/python3.6/site-packages/torch/nn/modules/module.py(529)__call__()\n",
      "-> if torch._C._get_tracing_state():\n",
      "(Pdb) s\n",
      "> /usr/local/lib64/python3.6/site-packages/torch/nn/modules/module.py(532)__call__()\n",
      "-> result = self.forward(*input, **kwargs)\n",
      "(Pdb) s\n",
      "--Call--\n",
      "> /usr/local/lib64/python3.6/site-packages/torch/nn/modules/adaptive.py(151)forward()\n",
      "-> def forward(self, input, target):\n",
      "(Pdb) s\n",
      "> /usr/local/lib64/python3.6/site-packages/torch/nn/modules/adaptive.py(152)forward()\n",
      "-> if input.size(0) != target.size(0):\n",
      "(Pdb) l\n",
      "147  \t        for i2h, h2o in self.tail:\n",
      "148  \t            i2h.reset_parameters()\n",
      "149  \t            h2o.reset_parameters()\n",
      "150  \t\n",
      "151  \t    def forward(self, input, target):\n",
      "152  ->\t        if input.size(0) != target.size(0):\n",
      "153  \t            raise RuntimeError('Input and target should have the same size '\n",
      "154  \t                               'in the batch dimension.')\n",
      "155  \t\n",
      "156  \t        used_rows = 0\n",
      "157  \t        batch_size = target.size(0)\n",
      "(Pdb) s\n",
      "> /usr/local/lib64/python3.6/site-packages/torch/nn/modules/adaptive.py(156)forward()\n",
      "-> used_rows = 0\n",
      "(Pdb) s\n",
      "> /usr/local/lib64/python3.6/site-packages/torch/nn/modules/adaptive.py(157)forward()\n",
      "-> batch_size = target.size(0)\n",
      "(Pdb) s\n",
      "> /usr/local/lib64/python3.6/site-packages/torch/nn/modules/adaptive.py(159)forward()\n",
      "-> output = input.new_zeros(batch_size)\n",
      "(Pdb) batch_size\n",
      "4\n",
      "(Pdb) s\n",
      "> /usr/local/lib64/python3.6/site-packages/torch/nn/modules/adaptive.py(160)forward()\n",
      "-> gather_inds = target.new_empty(batch_size)\n",
      "(Pdb) s\n",
      "> /usr/local/lib64/python3.6/site-packages/torch/nn/modules/adaptive.py(162)forward()\n",
      "-> cutoff_values = [0] + self.cutoffs\n",
      "(Pdb) s\n",
      "> /usr/local/lib64/python3.6/site-packages/torch/nn/modules/adaptive.py(163)forward()\n",
      "-> for i in range(len(cutoff_values) - 1):\n",
      "(Pdb) cutoff_values\n",
      "[0, 1, 99, 100]\n",
      "(Pdb) s\n",
      "> /usr/local/lib64/python3.6/site-packages/torch/nn/modules/adaptive.py(165)forward()\n",
      "-> low_idx = cutoff_values[i]\n",
      "(Pdb) i\n",
      "0\n",
      "(Pdb) s\n",
      "> /usr/local/lib64/python3.6/site-packages/torch/nn/modules/adaptive.py(166)forward()\n",
      "-> high_idx = cutoff_values[i + 1]\n",
      "(Pdb) s\n",
      "> /usr/local/lib64/python3.6/site-packages/torch/nn/modules/adaptive.py(168)forward()\n",
      "-> target_mask = (target >= low_idx) & (target < high_idx)\n",
      "(Pdb) low_idx\n",
      "0\n",
      "(Pdb) high_idx\n",
      "1\n",
      "(Pdb) target\n",
      "tensor([0.4340, 0.6639, 0.1833, 0.9387])\n",
      "(Pdb) (target >= low_idx) & (target < high_idx)\n",
      "tensor([True, True, True, True])\n",
      "(Pdb) exit\n",
      "> <ipython-input-474-1981a59aaafd>(66)forward()\n",
      "-> predicted_probs, loss = self.adaptive_softmax(predicted, target_probs)\n",
      "(Pdb) exit\n",
      "Falsifying example: test_summarize_net_returns_correct_shapes(\n",
      "    self=<__main__.TestNotebook testMethod=test_summarize_net_returns_correct_shapes>,\n",
      "    batch_size=4,\n",
      "    seq_len=100,\n",
      "    hidden_size=32,\n",
      "    num_layers=1,\n",
      "    vocabulary_size=100,\n",
      ")\n",
      "> <ipython-input-474-1981a59aaafd>(66)forward()\n",
      "-> predicted_probs, loss = self.adaptive_softmax(predicted, target_probs)\n",
      "(Pdb) exit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E\n",
      "======================================================================\n",
      "ERROR: test_summarize_net_returns_correct_shapes (__main__.TestNotebook)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-491-5db986a97df7>\", line 34, in test_summarize_net_returns_correct_shapes\n",
      "    st.sampled_from([4, 8, 12]),\n",
      "  File \"/usr/local/lib/python3.6/site-packages/hypothesis/core.py\", line 1055, in wrapped_test\n",
      "    state.run_engine()\n",
      "  File \"<ipython-input-491-5db986a97df7>\", line 54, in test_summarize_net_returns_correct_shapes\n",
      "    pred_probs, pred_modes, loss = model(embeddings, target, modes)\n",
      "  File \"/usr/local/lib64/python3.6/site-packages/torch/nn/modules/module.py\", line 532, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"<ipython-input-474-1981a59aaafd>\", line 66, in forward\n",
      "    predicted_probs, loss = self.adaptive_softmax(predicted, target_probs)\n",
      "  File \"<ipython-input-474-1981a59aaafd>\", line 66, in forward\n",
      "    predicted_probs, loss = self.adaptive_softmax(predicted, target_probs)\n",
      "  File \"/usr/lib64/python3.6/bdb.py\", line 51, in trace_dispatch\n",
      "    return self.dispatch_line(frame)\n",
      "  File \"/usr/lib64/python3.6/bdb.py\", line 70, in dispatch_line\n",
      "    if self.quitting: raise BdbQuit\n",
      "bdb.BdbQuit\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 637.957s\n",
      "\n",
      "FAILED (errors=1)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__' and RUN_TESTS:\n",
    "    import doctest\n",
    "    \n",
    "    doctest.testmod()\n",
    "    unittest.main(\n",
    "        argv=['first-arg-is-ignored'],\n",
    "        failfast=True,\n",
    "        exit=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not RUN_TESTS:\n",
    "    if 'trainer' in vars():\n",
    "        print(f\"About to delete old trainer\")\n",
    "        del trainer\n",
    "        \n",
    "    vocabulary = Vocabulary(nlp, [ articles[\"text\"], articles[\"headline\"] ])\n",
    "\n",
    "    trainer = InNotebookTrainer(\n",
    "        'test-run-1',\n",
    "        vocabulary,\n",
    "        articles,\n",
    "        optimizer_class_name='Adam',\n",
    "        model_args={\n",
    "            'hidden_size': 128,\n",
    "            'input_size': 300,\n",
    "            'num_layers': 2,\n",
    "            'vocabulary_size': len(vocabulary)\n",
    "        },\n",
    "        optimizer_args={},\n",
    "        batch_size=32,\n",
    "        update_every=1,\n",
    "        probability_of_mask_for_word=0.2,\n",
    "        device=torch.device('cuda')\n",
    "    )\n",
    "\n",
    "    trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
