{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import functools\n",
    "import statistics\n",
    "import itertools\n",
    "import random\n",
    "import math\n",
    "from pathlib import Path\n",
    "import pdb\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "import tensorwatch as tw\n",
    "\n",
    "import spacy\n",
    "from cached_property import cached_property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'nlp' not in vars():\n",
    "    nlp = spacy.load(\n",
    "        \"en_core_web_lg\",\n",
    "        disable=[\"tagger\", \"ner\", \"textcat\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'articles' not in vars():\n",
    "    articles = pd.read_parquet(\"data/articles-processed.parquet.gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNModel(nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__()\n",
    "        \n",
    "        self._args = args\n",
    "        self._kwargs = kwargs\n",
    "        \n",
    "    def save(self, path):\n",
    "        torch.save(\n",
    "            {\n",
    "                'state': self.state_dict(),\n",
    "                'args': self._args,\n",
    "                'kwargs': self._kwargs\n",
    "            },\n",
    "            path\n",
    "        )\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, path):\n",
    "        if Path(path).exists():\n",
    "            data = torch.load(path)\n",
    "\n",
    "            model = cls(*data['args'], **data['kwargs'])\n",
    "            model.load_state_dict(checkpoint['state'])\n",
    "\n",
    "            return model\n",
    "        else:\n",
    "            raise FileNotFoundError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscriminatorNet(NNModel):\n",
    "    def __init__(self):\n",
    "        super(DiscriminatorNet, self).__init__()\n",
    "\n",
    "    def forward(self, hidden_state):\n",
    "        \"\"\"\n",
    "        The forward pass for the network\n",
    "        \n",
    "        hidden_state : tensor (batch_num, hidden_size)\n",
    "        \n",
    "        returns         : tensor (batch_num, 1)\n",
    "        \"\"\"\n",
    "        \n",
    "        return hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SummarizeNet(NNModel):\n",
    "    def __init__(self, hidden_size, input_size, num_layers):\n",
    "        super(SummarizeNet, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.gru = nn.GRU(\n",
    "            input_size,\n",
    "            hidden_size,\n",
    "            num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        \n",
    "        self.discriminate = DiscriminatorNet()\n",
    "\n",
    "    def forward(self, word_embeddings, modes):\n",
    "        \"\"\"\n",
    "        The forward pass for the network\n",
    "        \n",
    "        word_embeddings : tensor (batch_num, max_seq_len, embedding_length)\n",
    "        \n",
    "        returns         : tuple (\n",
    "                            tensor (batch_num, max_seq_len, vocab_len),\n",
    "                            tensor (batch_num, hidden_size)\n",
    "                          )\n",
    "        \n",
    "        First tensor in the returning tuple are predicted word embeddings\n",
    "        The second tensor are probabilities of the output being a headline\n",
    "        \"\"\"\n",
    "        \n",
    "        return word_embeddings, modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArticlesDataset(Dataset):\n",
    "    def __init__(self, dataframe, mode, transforms=[]):\n",
    "        self.data = dataframe[dataframe.set == mode]\n",
    "        self.transforms = transforms\n",
    "        self.mode = mode\n",
    "        \n",
    "    def __len__(self):\n",
    "        return 2*len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        _idx = []\n",
    "        \n",
    "        if torch.is_tensor(idx):\n",
    "            _idx = idx.tolist()\n",
    "        \n",
    "        if isinstance(idx, list):\n",
    "            _idx = idx\n",
    "        else:\n",
    "            _idx = [ idx ]\n",
    "        \n",
    "        _ids = [ (i - (i % 2))/2 for i in _idx]\n",
    "\n",
    "        data = self.data.iloc[_ids, :]\n",
    "        data['asked_id'] = _idx\n",
    "        \n",
    "        data = pd.DataFrame(\n",
    "            {\n",
    "                'set': [self.mode for _ in range(0, len(_ids))],\n",
    "                'mode': np.array([ (0 if i % 2 == 0 else 1) for i in _idx ]),\n",
    "                'text': data.apply(lambda row: row['text'] if row['asked_id'] % 2 == 0 else row['headline'], axis=1),\n",
    "                'title': data['normalized_title']\n",
    "            }\n",
    "        )\n",
    "\n",
    "        for transform in self.transforms:\n",
    "            data = transform(data)\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextToParsedDoc(object):\n",
    "    def __init__(self, nlp):\n",
    "        self.nlp = nlp\n",
    "        \n",
    "    def __call__(self, sample):\n",
    "        sample['doc'] = sample.apply(lambda row: self.nlp(row['text']), axis=1)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordsToVectors(object):\n",
    "    def __init__(self, nlp):\n",
    "        self.nlp = nlp\n",
    "        \n",
    "    def document_embeddings(self, doc):\n",
    "        word_embeddings = [\n",
    "            [ l.vector ] if l.whitespace_ == '' else [ l.vector, np.zeros_like(l.vector) ] for l in doc\n",
    "        ]\n",
    "\n",
    "        return np.stack(\n",
    "            [\n",
    "                vector for vectors in word_embeddings for vector in vectors\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        \n",
    "        sample['word_embeddings'] = sample.apply(\n",
    "            lambda row: self.document_embeddings(row['doc']),\n",
    "            axis=1\n",
    "        )\n",
    "    \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddNoiseToEmbeddings(object):\n",
    "    def __init__(self, probability_of_mask_for_word):\n",
    "        self.probability_of_mask_for_word = probability_of_mask_for_word\n",
    "        self.rng = np.random.default_rng()\n",
    "        \n",
    "    def mask_vector(self, vector):\n",
    "        \"\"\"\n",
    "        Masks words with zeros randomly\n",
    "        \"\"\"\n",
    "        seq_len = vector.shape[0]\n",
    "        vector_len = vector.shape[1]\n",
    "        \n",
    "        mask = np.repeat(\n",
    "            self.rng.choice(\n",
    "                [0, 1],\n",
    "                seq_len,\n",
    "                p=[\n",
    "                    self.probability_of_mask_for_word,\n",
    "                    (1 - self.probability_of_mask_for_word)\n",
    "                ]\n",
    "            ).reshape((seq_len, 1)),\n",
    "            vector_len,\n",
    "            axis=1\n",
    "        )\n",
    "        \n",
    "        return vector * mask\n",
    "        \n",
    "    def __call__(self, sample):       \n",
    "        sample['noisy_word_embeddings'] = sample['word_embeddings'].apply(self.mask_vector)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MergeBatch(object):\n",
    "    def stack_vectors(self, vectors):\n",
    "        max_seq = max([vector.shape[0] for vector in vectors])\n",
    "        \n",
    "        return np.stack(\n",
    "            [\n",
    "                np.pad(vector, [(0, max_seq - vector.shape[0]), (0, 0)])\n",
    "                for vector in vectors\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "    def __call__(self, sample):\n",
    "        del sample['doc']\n",
    "        \n",
    "        sample = sample.to_dict(orient=\"list\")\n",
    "        \n",
    "        sample['word_embeddings'] = torch.from_numpy(\n",
    "            self.stack_vectors(sample['word_embeddings'])\n",
    "        )\n",
    "        sample['noisy_word_embeddings'] = torch.from_numpy(\n",
    "            self.stack_vectors(sample['noisy_word_embeddings'])\n",
    "        )\n",
    "        sample['mode'] = torch.from_numpy(\n",
    "            np.stack(sample['mode'])\n",
    "        )\n",
    "    \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SetAllToSummarizing(object):\n",
    "    def __call__(self, sample):\n",
    "        sample['mode'] = np.ones_like(sample['mode'])\n",
    "        \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader(object):\n",
    "    def __init__(self, dataset, batch_size=8, num_workers=1):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        \n",
    "    @property\n",
    "    def epoch_size(self):\n",
    "        return math.ceil(len(self.dataset) / self.batch_size) * self.batch_size\n",
    "    \n",
    "    def __iter__(self):\n",
    "        ids = random.choices(range(0, len(self.dataset)), k=self.epoch_size)\n",
    "        \n",
    "        for start_ix in range(0, self.epoch_size, self.batch_size):\n",
    "            yield self.dataset[ids[start_ix:(start_ix + self.batch_size)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArticlesBatch:\n",
    "    def __init__(self, data, id):\n",
    "        self.data = data\n",
    "        self.id = id\n",
    "        \n",
    "    def __getattr__(self, name):\n",
    "        if name in self.data:\n",
    "            return self.data[name]\n",
    "        else:\n",
    "            raise AttributeError(f\"Attribute missing: {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(object):\n",
    "    def __init__(self, nlp):\n",
    "        self.nlp = nlp\n",
    "        \n",
    "    def decode_embeddings(self, word_embeddings):\n",
    "        return np.apply_along_axis(self.decode_embeddings_1d, 0, word_embeddings)\n",
    "        \n",
    "    def decode_embeddings_1d(self, word_embeddings):\n",
    "        \"\"\"\n",
    "        Decodes a single document. Word embeddings given are of shape (N, D)\n",
    "        where N is the number of lexemes and D the dimentionality of the embedding vector\n",
    "        \"\"\"\n",
    "        \n",
    "        return \"\".join(\n",
    "            [\n",
    "                token.text.lower() if not token.is_oov else \" \"\n",
    "                for token in [\n",
    "                    self.nlp.vocab[ks[0]]\n",
    "                    for ks in self.nlp.vocab.vectors.most_similar(\n",
    "                        word_embeddings, n=1\n",
    "                    )[0]\n",
    "                ]\n",
    "            ]\n",
    "        ).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metrics(object):\n",
    "    def __init__(self, loss):\n",
    "        self.losses = [loss]\n",
    "    \n",
    "    @classmethod\n",
    "    def empty(cls, mode):\n",
    "        return cls(mode)\n",
    "    \n",
    "    @property\n",
    "    def last_loss(self):\n",
    "        return self.losses[len(self.losses) - 1]\n",
    "    \n",
    "    def running_mean_loss(self, n=100):\n",
    "        cumsum = numpy.cumsum(numpy.insert(np.array(self.losses), 0, 0)) \n",
    "        return (cumsum[n:] - cumsum[:-n]) / float(n)\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        self.losses += other.losses\n",
    "        \n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpdateInfo(object):\n",
    "    def __init__(self, decoder, batch, word_embeddings, loss_sum, mode):\n",
    "        self.decoder = decoder\n",
    "        self.batch = batch\n",
    "        self.word_embeddings = word_embeddings\n",
    "        self.loss_sum = loss_sum\n",
    "        self.mode = mode\n",
    "        \n",
    "    @cached_property\n",
    "    def decoded_inferred_texts(self):\n",
    "        return self.decoder.decode_embeddings(self.word_embeddings)\n",
    "    \n",
    "    @cached_property\n",
    "    def metrics(self):\n",
    "        return Metrics(self.mode, self.loss_sum)\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"{self.mode} | {self.batch.id}\\t| Loss: {loss_sum}\\t\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseTrainer:\n",
    "    def __init__(self, name, nlp, dataframe,\n",
    "                 optimizer_class_name,\n",
    "                 model_args, optimizer_args, \n",
    "                 batch_size, update_every, loader_workers,\n",
    "                 probability_of_mask_for_word\n",
    "                ):\n",
    "        self.name = name\n",
    "        \n",
    "        self.datasets = {\n",
    "            \"train\": ArticlesDataset(\n",
    "                dataframe,\n",
    "                \"train\",\n",
    "                transforms=[\n",
    "                    TextToParsedDoc(nlp),\n",
    "                    WordsToVectors(nlp),\n",
    "                    AddNoiseToEmbeddings(probability_of_mask_for_word),\n",
    "                    MergeBatch()\n",
    "                ]\n",
    "            ),\n",
    "            \"test\":  ArticlesDataset(\n",
    "                dataframe,\n",
    "                \"test\",\n",
    "                transforms=[\n",
    "                    TextToParsedDoc(nlp),\n",
    "                    WordsToVectors(nlp),\n",
    "                    SetAllToSummarizing(),\n",
    "                    MergeBatch()\n",
    "                ]\n",
    "            ),\n",
    "            \"eval\":  ArticlesDataset(\n",
    "                dataframe,\n",
    "                \"eval\",\n",
    "                transforms=[\n",
    "                    TextToParsedDoc(nlp),\n",
    "                    WordsToVectors(nlp),\n",
    "                    MergeBatch()\n",
    "                ]\n",
    "            )\n",
    "        }\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.update_every = update_every\n",
    "        self.loader_workers = loader_workers\n",
    "        \n",
    "        self.optimizer_class_name = optimizer_class_name\n",
    "        \n",
    "        self.model_args = model_args\n",
    "        self.optimizer_args = optimizer_args\n",
    "        \n",
    "        self.current_batch_id = 0\n",
    "        \n",
    "        self.decoder = Decoder(nlp)\n",
    "        \n",
    "        if self.has_checkpoint:\n",
    "            self.load_last_checkpoint()\n",
    "        \n",
    "    @cached_property\n",
    "    def model(self):\n",
    "        try:\n",
    "            return SummarizeNet.load(f\"{self.checkpoint_path}/model.pth\")\n",
    "        except FileNotFoundError:\n",
    "            return SummarizeNet(**self.model_args)\n",
    "        \n",
    "    @cached_property\n",
    "    def optimizer(self):\n",
    "        class_ = getattr(torch.optim, self.optimizer_class_name)\n",
    "        \n",
    "        return class_(self.model.parameters(), **self.optimizer_args)\n",
    "    \n",
    "    @property\n",
    "    def checkpoint_path(self):\n",
    "        return f\"checkpoints/{self.name}/batch-#{self.current_batch_id}\"\n",
    "    \n",
    "    def save_checkpoint(self):\n",
    "        os.makedirs(self.checkpoint_path, exist_ok=True)\n",
    "        \n",
    "        self.model.save(f\"{self.checkpoint_path}/model.pth\")\n",
    "        \n",
    "        torch.save(\n",
    "            {\n",
    "                'current_batch_id': self.current_batch_id,\n",
    "                'batch_size': self.batch_size,\n",
    "                'update_every': self.update_every,\n",
    "                'optimizer_class_name': self.optimizer_class_name,\n",
    "                'optimizer_args': self.optimizer_args,\n",
    "                'optimizer_state_dict': self.optimizer.state_dict()\n",
    "            },\n",
    "            f\"{self.checkpoint_path}/trainer.pth\"\n",
    "        )\n",
    "    \n",
    "    @property\n",
    "    def checkpoint_directories(self):\n",
    "        return sorted(Path(\".\").glob(f\"checkpoints/{self.name}/batch-*\"), reverse=True)\n",
    "    \n",
    "    @property\n",
    "    def has_checkpoint(self):\n",
    "        return len(self.checkpoint_directories) > 0\n",
    "    \n",
    "    def load_last_checkpoint(self):\n",
    "        path = self.checkpoint_directories[0]\n",
    "        \n",
    "        data = torch.load(f\"{path}/trainer.pth\")\n",
    "        \n",
    "        self.batch_size = data['current_batch_id']\n",
    "        self.update_every = data['update_every']\n",
    "        self.loader_workers = data['loader_workers']\n",
    "        \n",
    "        self.optimizer_class_name = data['optimizer_class_name']\n",
    "        self.optimizer_args = data['optimizer_args']\n",
    "        \n",
    "        self.current_batch_id = data['current_batch_id']\n",
    "        \n",
    "        del self.__dict__['model']\n",
    "        del self.__dict__['optimizer']\n",
    "        \n",
    "        self.optimizer.load_state_dict(data['optimizer_state_dict'])\n",
    "    \n",
    "    def batches(self, mode):\n",
    "        start_id = self.current_batch_id\n",
    "        \n",
    "        while True:\n",
    "            loader = DataLoader(\n",
    "                self.datasets[mode],\n",
    "                batch_size=self.batch_size,\n",
    "                num_workers=self.loader_workers\n",
    "            )\n",
    "\n",
    "            for ix, data in enumerate(loader):\n",
    "                self.current_batch_id += ix\n",
    "                \n",
    "                yield(\n",
    "                    ArticlesBatch(\n",
    "                        data,\n",
    "                        id=self.current_batch_id\n",
    "                    )\n",
    "                )\n",
    "    \n",
    "    def work_batch(self, batch):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def updates(self, mode=\"train\", update_every=None):\n",
    "        batches = self.batches(mode)\n",
    "        loss_sum = 0\n",
    "        \n",
    "        if update_every is None:\n",
    "            update_every = self.update_every\n",
    "        \n",
    "        for batch in batches:\n",
    "            if mode == \"train\":\n",
    "                self.model.train()\n",
    "            else:\n",
    "                self.model.eval()\n",
    "            \n",
    "            loss, word_embeddings = self.work_batch(batch)\n",
    "            loss /= self.update_every * self.batch_size\n",
    "            \n",
    "            loss.backward()\n",
    "            loss_sum += loss\n",
    "            \n",
    "            # we're doing the accumulated gradients trick to get the gradients variance\n",
    "            # down while being able to use commodity GPU:\n",
    "            if batch.id % update_every == 0:\n",
    "                if mode == \"train\":\n",
    "                    self.optimizer.step()\n",
    "                    self.optimizer.zero_grad()\n",
    "                \n",
    "                yield(UpdateInfo(self.decoder, batch, word_embeddings, loss_sum, mode=mode))\n",
    "                \n",
    "                loss_sum = 0\n",
    "    \n",
    "    def train_and_evaluate_updates(self, evaluate_every=100):\n",
    "        train_updates = self.updates(mode=\"train\")\n",
    "        evaluate_updates = self.updates(mode=\"eval\")\n",
    "        \n",
    "        for update_info in train_updates:\n",
    "            yield(update_info)\n",
    "            \n",
    "            if update_info.batch.id % evaluate_every == 0:\n",
    "                yield(next(evaluate_updates))\n",
    "    \n",
    "    def test_updates(self):\n",
    "        return self.updates(mode=\"test\", update_every=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(BaseTrainer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(Trainer, self).__init__(*args, **kwargs)\n",
    "        \n",
    "    def compute_loss(self, word_embeddings, original_word_embeddings, discriminate_probs):\n",
    "        embeddings_loss = F.cosine_embedding_loss(\n",
    "            word_embeddings,\n",
    "            original_word_embeddings,\n",
    "            torch.ones(word_embeddings.shape[0])\n",
    "        )\n",
    "        \n",
    "        discriminator_loss = F.binary_cross_entropy(\n",
    "            discriminate_probs,\n",
    "            torch.zeros_like(discriminate_probs)\n",
    "        )\n",
    "        \n",
    "        return embeddings_loss + discriminator_loss\n",
    "        \n",
    "\n",
    "    def work_batch(self, batch):\n",
    "        word_embeddings, discriminate_probs = self.model(\n",
    "            batch.noisy_word_embeddings,\n",
    "            batch.mode\n",
    "        )\n",
    "\n",
    "        # we're diverging from the article here by outputting the word embeddings\n",
    "        # instead of the probabilities for each word in a vocabulary\n",
    "        # our loss function is using the cosine embedding loss coupled with\n",
    "        # the discriminator loss:\n",
    "        return (\n",
    "            self.compute_loss(word_embeddings, batch.word_embeddings, discriminate_probs),\n",
    "            word_embeddings\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InNotebookTrainer(Trainer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(InNotebookTrainer, self).__init__(*args, **kwargs)\n",
    "        \n",
    "        self.watcher = tw.Watcher(filename=f\"{self.name}.log\")\n",
    "        \n",
    "        self.train_losses_stream = self.watcher.create_stream('train_losses')\n",
    "        self.train_moving_losses_stream = self.watcher.create_stream('train_moving_losses')\n",
    "        \n",
    "        self.eval_losses_stream = self.watcher.create_stream('eval_losses')\n",
    "        \n",
    "        self.test_texts_stream = self.watcher.create_stream('test_texts')\n",
    "        \n",
    "    def __del__(self):\n",
    "        print(f\"Closing the watcher\")\n",
    "        self.watcher.close()\n",
    "        \n",
    "    def __enter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __exit__(self, exception_type, exception_value, traceback):\n",
    "        if not self.watcher.closed:\n",
    "            self.watcher.close()\n",
    "\n",
    "    def train(self):\n",
    "        test_updates = self.test_updates()\n",
    "        \n",
    "        cumulative_train_metrics = Metrics.empty(mode=\"train\")\n",
    "        cumulative_evaluate_metrics = Metrics.empty(mode=\"eval\")\n",
    "\n",
    "        for update_info in self.train_and_evaluate_updates():\n",
    "            if update_info.from_train:\n",
    "                cumulative_train_metrics += update_info.metrics\n",
    "                \n",
    "                print(f\"{update_info.metrics}\")\n",
    "\n",
    "                self.train_losses_stream.write((update_info.batch.id, update_info.metrics.last_loss))\n",
    "                self.train_moving_losses_stream.write((update_info.batch.id, update_info.metrics.running_mean_loss(n=100)))\n",
    "\n",
    "            if update_info.from_evaluate:\n",
    "                cumulative_evaluate_metrics += update_info.metrics\n",
    "\n",
    "                self.eval_losses_stream.write((update_info.batch.id, update_info.metrics.loss))\n",
    "\n",
    "                self.save_checkpoint()\n",
    "                \n",
    "            if update_info.batch.id % 1000 == 0:\n",
    "                test_update = next(test_updates)\n",
    "                \n",
    "                self.test_texts_stream.write(\n",
    "                    (\n",
    "                        update_info.batch.text,\n",
    "                        update_info.decoded_inferred_texts\n",
    "                    )\n",
    "                )\n",
    "                \n",
    "    def test(self):\n",
    "        cumulative_metrics = Metrics.empty(mode=\"test\")\n",
    "        \n",
    "        for update_info in self.test_updates():\n",
    "            cumulative_metrics += update_info.metrics\n",
    "            \n",
    "            self.test_texts_stream.write(\n",
    "                (\n",
    "                    update_info.batch.text,\n",
    "                    update_info.decoded_inferred_texts\n",
    "                )\n",
    "            )\n",
    "\n",
    "        print(cumulative_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Debug][0]: FileStream started : /home/kamil/projects/nlp-article/test-run-1.log : t=14.74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected np.ndarray (got list)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-fb296b842b07>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m )\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-20-e5fd3c70c587>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mcumulative_evaluate_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"eval\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mupdate_info\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_and_evaluate_updates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mupdate_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_train\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                 \u001b[0mcumulative_train_metrics\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mupdate_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-2dfa2a487741>\u001b[0m in \u001b[0;36mtrain_and_evaluate_updates\u001b[0;34m(self, evaluate_every)\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0mevaluate_updates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"eval\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mupdate_info\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_updates\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m             \u001b[0;32myield\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupdate_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-2dfa2a487741>\u001b[0m in \u001b[0;36mupdates\u001b[0;34m(self, mode, update_every)\u001b[0m\n\u001b[1;32m    147\u001b[0m             \u001b[0mupdate_every\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_every\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"train\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-2dfa2a487741>\u001b[0m in \u001b[0;36mbatches\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    127\u001b[0m             )\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_batch_id\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-84c798b11500>\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mstart_ix\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart_ix\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_ix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-42c444285957>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtransform\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-527a3e09fd9a>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, sample)\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack_vectors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'noisy_word_embeddings'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         )\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mode'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mode'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected np.ndarray (got list)"
     ]
    }
   ],
   "source": [
    "if 'trainer' in vars():\n",
    "    print(f\"About to delete old trainer\")\n",
    "    del trainer\n",
    "\n",
    "trainer = InNotebookTrainer(\n",
    "    'test-run-1',\n",
    "    nlp,\n",
    "    articles,\n",
    "    optimizer_class_name='Adam',\n",
    "    model_args={\n",
    "        'hidden_size': 128,\n",
    "        'input_size': 300,\n",
    "        'num_layers': 2\n",
    "    },\n",
    "    optimizer_args={},\n",
    "    batch_size=8,\n",
    "    update_every=100,\n",
    "    loader_workers=1,\n",
    "    probability_of_mask_for_word=0.2\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
