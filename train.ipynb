{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import functools\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import spacy\n",
    "from cached_property import cached_property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\n",
    "    \"en_core_web_lg\",\n",
    "    disable=[\"tagger\", \"ner\", \"textcat\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = pd.read_parquet(\"data/articles-processed.parquet.gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscriminatorNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DiscriminatorNet, self).__init__()\n",
    "\n",
    "    def forward(self, hidden_state):\n",
    "        \"\"\"\n",
    "        The forward pass for the network\n",
    "        \n",
    "        hidden_state : tensor (batch_num, hidden_size)\n",
    "        \n",
    "        returns         : tensor (batch_num, 1)\n",
    "        \"\"\"\n",
    "        \n",
    "        return hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SummarizeNet(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(SummarizeNet, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "    def forward(self, word_embeddings, generate=\"sentence\"):\n",
    "        \"\"\"\n",
    "        The forward pass for the network\n",
    "        \n",
    "        word_embeddings : tensor (batch_num, max_seq_len, vocab_len)\n",
    "        \n",
    "        returns         : tuple (\n",
    "                            tensor (batch_num, max_seq_len, vocab_len),\n",
    "                            tensor (batch_num, hidden_size)\n",
    "                          )\n",
    "        \n",
    "        First tensor in the returning tuple is a probability over the vocabulary\n",
    "        for each sequence position\n",
    "        \n",
    "        The second tensor is an encoder's hidden state \n",
    "        \"\"\"\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArticlesDataset(Dataset):\n",
    "    def __init__(self, dataframe, mode, transforms=[]):\n",
    "        self.data = dataframe\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        data = self.data.iloc[idx, :]\n",
    "\n",
    "        for transform in self.transforms:\n",
    "            data = transform(data)\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordsToVectors(object):\n",
    "    def __init__(self, nlp):\n",
    "        self.nlp = nlp\n",
    "        \n",
    "    def __call__(self, sample):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddNoiseToEmbeddings(object):\n",
    "    def __init__(self, probability_of_mask_for_word, probability_of_masking_for_sample):\n",
    "        self.probability_of_mask_for_word = probability_of_mask_for_word\n",
    "        self.probability_of_masking_for_sample = probability_of_masking_for_sample\n",
    "        \n",
    "    def __call__(self, sample):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArticlesBatch:\n",
    "    def __init__(self, data, id):\n",
    "        self.data = data\n",
    "        self.id = id\n",
    "        \n",
    "    def __getattr__(self, name):\n",
    "        if name in self.data:\n",
    "            return self.data[name]\n",
    "        else:\n",
    "            raise AttributeError(f\"Attribute missing: {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseTrainer:\n",
    "    def __init__(self, name, nlp, summarize_model, discriminate_model, dataframe,\n",
    "                 batch_size, update_every, save_every, loader_workers,\n",
    "                 probability_of_mask_for_word, probability_of_masking_for_sample,\n",
    "                 lambda_article, lambda_sentence):\n",
    "        self.name = name\n",
    "        \n",
    "        self.datasets = {\n",
    "            \"train\": ArticlesDataset(\n",
    "                dataframe,\n",
    "                \"train\",\n",
    "                transforms=[\n",
    "                    WordsToVectors(nlp),\n",
    "                    AddNoiseToEmbeddings(\n",
    "                        probability_of_mask_for_word,\n",
    "                        probability_of_masking_for_sample\n",
    "                    )\n",
    "                ]\n",
    "            ),\n",
    "            \"test\":  ArticlesDataset(\n",
    "                dataframe,\n",
    "                \"test\",\n",
    "                transforms=[\n",
    "                    WordsToVectors(nlp)\n",
    "                ]\n",
    "            ),\n",
    "            \"eval\":  ArticlesDataset(\n",
    "                dataframe,\n",
    "                \"eval\",\n",
    "                transforms=[\n",
    "                    WordsToVectors(nlp)\n",
    "                ]\n",
    "            )\n",
    "        }\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.update_every = update_every\n",
    "        self.save_every = save_every\n",
    "        self.loader_workers = loader_workers\n",
    "        \n",
    "        self.summarize_model = summarize_model\n",
    "        self.discriminate_model = discriminate_model\n",
    "        \n",
    "        self.lambda_article = lambda_article\n",
    "        self.lambda_sentence = lambda_sentence\n",
    "        \n",
    "        self.current_batch_id = 0\n",
    "        \n",
    "    @property\n",
    "    def models(self):\n",
    "        return self.summarize_model, self.discriminate_model\n",
    "    \n",
    "    def save(self):\n",
    "        checkpoint_path = f\"checkpoints/{self.name}/batch-#{self.current_batch_id}\"\n",
    "        os.makedirs(checkpoint_path, exist_ok=True)\n",
    "        \n",
    "        torch.save(\n",
    "            {\n",
    "                'current_batch_id': self.current_batch_id,\n",
    "                'batch_size': self.batch_size,\n",
    "                'update_every': self.update_every,\n",
    "                'save_every': self.save_every,\n",
    "                'lambda_article': self.lambda_article,\n",
    "                'lambda_sentence': self.lambda_sentence,\n",
    "                'summarize_model_state': self.summarize_model.state_dict(),\n",
    "                'discriminate_model_state': self.discriminate_model.state_dict(),\n",
    "                'optimizer_state_dict': self.optimizer.state_dict()\n",
    "            },\n",
    "            f\"{checkpoint_path}/state.pth\"\n",
    "        )\n",
    "        \n",
    "    def load(name, dataframe):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def batches(self, mode):\n",
    "        start_id = self.current_batch_id\n",
    "        \n",
    "        while True:\n",
    "            loader = DataLoader(\n",
    "                self.datasets[mode],\n",
    "                batch_size=self.batch_size,\n",
    "                shuffle=True,\n",
    "                num_workers=self.loader_workers\n",
    "            )\n",
    "\n",
    "            for ix, data in enumerate(loader):\n",
    "                self.current_batch_id += ix\n",
    "                \n",
    "                yield(\n",
    "                    ArticlesBatch(\n",
    "                        data,\n",
    "                        id=self.current_batch_id\n",
    "                    )\n",
    "                )\n",
    "        \n",
    "    \n",
    "    def batch_loss(self, batch):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def after_update(self, batch, loss_sum):\n",
    "        pass\n",
    "    \n",
    "    def train(self):\n",
    "        batches = self.batches(\"train\")\n",
    "        loss_sum = 0\n",
    "        \n",
    "        for batch in batches:\n",
    "            loss = self.batch_loss(batch) / (self.update_every * self.batch_size)\n",
    "            \n",
    "            loss.backward()\n",
    "            loss_sum += loss\n",
    "            \n",
    "            # we're doing the accumulated gradients trick to get the gradients variance\n",
    "            # down while being able to use commodity GPU:\n",
    "            if batch.id % self.update_every == 0:\n",
    "                self.optimizer.step()\n",
    "                self.optimizer.zero_grad()\n",
    "                \n",
    "                self.after_update(batch, loss_sum)\n",
    "                \n",
    "                loss_sum = 0\n",
    "    \n",
    "    def test(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def evaluate(self):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(BaseTrainer):\n",
    "    def __init__(self, name, nlp, summarize_model, discriminate_model, dataframe,\n",
    "                 batch_size, update_every, save_every, loader_workers,\n",
    "                 probability_of_mask_for_word, probability_of_masking_for_sample,\n",
    "                 lambda_article, lambda_sentence):\n",
    "        super().__init__(\n",
    "            name, nlp,\n",
    "            summarize_model, discriminate_model, dataframe,\n",
    "            batch_size, update_every, save_every, loader_workers,\n",
    "            lambda_article, lambda_sentence\n",
    "        )\n",
    "        \n",
    "    def compute_loss(self, articles_word_embeddings, orig_articles_word_embeddings,\n",
    "                     sentences_word_embeddings, orig_sentences_word_embeddings,\n",
    "                     sentences_numbers_in_articles,\n",
    "                     discriminate_articles_probs,\n",
    "                     discriminate_sentences_probs\n",
    "                    ):\n",
    "        articles_loss = F.cosine_embedding_loss(\n",
    "            articles_word_embeddings,\n",
    "            orig_articles_word_embeddings,\n",
    "            torch.ones(articles_word_embeddings.shape[0])\n",
    "        )\n",
    "        \n",
    "        sentences_loss = F.cosine_embedding_loss(\n",
    "            sentences_word_embeddings,\n",
    "            orig_sentences_word_embeddings,\n",
    "            torch.ones(articles_word_embeddings.shape[0])\n",
    "        ) / sentences_numbers_in_articles\n",
    "        \n",
    "        discriminator_articles_loss = F.binary_cross_entropy(\n",
    "            discriminate_articles_probs,\n",
    "            torch.zeros_like(discriminate_articles_probs)\n",
    "        )\n",
    "        \n",
    "        discriminator_sentences_loss = F.binary_cross_entropy(\n",
    "            discriminate_sentences_probs,\n",
    "            torch.zeros_like(discriminate_sentences_probs)\n",
    "        )\n",
    "        \n",
    "        return (articles_loss * self.lambda_article).sum(dim=0) +\n",
    "               (sentences_loss * self.lambda_sentence).sum(dim=0) +\n",
    "               discriminator_articles_loss +\n",
    "               discriminator_sentences_loss\n",
    "        \n",
    "\n",
    "    def batch_loss(self, batch):\n",
    "        # article -> article (de-noising)\n",
    "        articles_word_embeddings, articles_state = self.summarize_model(\n",
    "            batch.articles_noisy_word_embeddings,\n",
    "            generate=\"article\"\n",
    "        )\n",
    "\n",
    "        # sentence -> sentence (de-noising)\n",
    "        sentences_word_embeddings, sentences_state = self.summarize_model(\n",
    "            batch.sentences_noisy_word_embeddings,\n",
    "            generate=\"sentence\"\n",
    "        )\n",
    "\n",
    "        # the discriminator guessing which mode (article or sentence) was one state\n",
    "        # created for to deal with the \"segregation\" problem described in the paper:\n",
    "        discriminate_articles_probs = self.discriminate_model(articles_state)\n",
    "        discriminate_sentences_probs = self.discriminate_model(sentences_state)\n",
    "\n",
    "        # we're diverging from the article here by outputting the word embeddings\n",
    "        # instead of the probabilities for each word in a vocabulary\n",
    "        # our loss function is using the cosine embedding loss coupled with\n",
    "        # the discriminator loss:\n",
    "        return self.compute_loss(\n",
    "            articles_word_embeddings,\n",
    "            batch.articles_word_embeddings,\n",
    "\n",
    "            sentences_word_embeddings,\n",
    "            batch.sentences_word_embeddings,\n",
    "            batch.sentences_numbers_in_articles,\n",
    "\n",
    "            discriminate_articles_probs,\n",
    "            discriminate_sentences_probs\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(articles, 8)\n",
    "\n",
    "for epoch in trainer.train():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ArticlesDataset(\n",
    "    articles,\n",
    "    mode=\"train\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SummarizeNet()\n"
     ]
    }
   ],
   "source": [
    "print(SummarizeNet())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "headline            \\nGet a bachelorâ€™s degree.,\\nEnroll in a studi...\n",
       "text                 It is possible to become a VFX artist without...\n",
       "normalized_title                         HowtoBeaVisualEffectsArtist1\n",
       "set                                                             train\n",
       "Name: 2, dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in [t for t in list(nlp(\"Lorem ipsum dolor sit amet, consectetur adipiscing elit. YYy yyyy\").sents)[0]]:\n",
    "#     print(f\"=> {i.vector}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
