{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import functools\n",
    "import itertools\n",
    "import random\n",
    "import math\n",
    "from pathlib import Path\n",
    "import pdb\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import spacy\n",
    "from cached_property import cached_property\n",
    "\n",
    "from rouge import Rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\n",
    "    \"en_core_web_lg\",\n",
    "    disable=[\"tagger\", \"ner\", \"textcat\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = pd.read_parquet(\"data/articles-processed.parquet.gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNModel(nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__()\n",
    "        \n",
    "        self._args = args\n",
    "        self._kwargs = kwargs\n",
    "        \n",
    "    def save(self, path):\n",
    "        torch.save(\n",
    "            {\n",
    "                'state': self.state_dict(),\n",
    "                'args': self._args,\n",
    "                'kwargs': self._kwargs\n",
    "            },\n",
    "            path\n",
    "        )\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, path):\n",
    "        if Path(path).exists():\n",
    "            data = torch.load(path)\n",
    "\n",
    "            model = cls(*data['args'], **data['kwargs'])\n",
    "            model.load_state_dict(checkpoint['state'])\n",
    "\n",
    "            return model\n",
    "        else:\n",
    "            raise FileNotFoundError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscriminatorNet(NNModel):\n",
    "    def __init__(self):\n",
    "        super(DiscriminatorNet, self).__init__()\n",
    "\n",
    "    def forward(self, hidden_state):\n",
    "        \"\"\"\n",
    "        The forward pass for the network\n",
    "        \n",
    "        hidden_state : tensor (batch_num, hidden_size)\n",
    "        \n",
    "        returns         : tensor (batch_num, 1)\n",
    "        \"\"\"\n",
    "        \n",
    "        return hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SummarizeNet(NNModel):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(SummarizeNet, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "    def forward(self, word_embeddings, generate=\"sentence\"):\n",
    "        \"\"\"\n",
    "        The forward pass for the network\n",
    "        \n",
    "        word_embeddings : tensor (batch_num, max_seq_len, vocab_len)\n",
    "        \n",
    "        returns         : tuple (\n",
    "                            tensor (batch_num, max_seq_len, vocab_len),\n",
    "                            tensor (batch_num, hidden_size)\n",
    "                          )\n",
    "        \n",
    "        First tensor in the returning tuple is a probability over the vocabulary\n",
    "        for each sequence position\n",
    "        \n",
    "        The second tensor is an encoder's hidden state \n",
    "        \"\"\"\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArticlesDataset(Dataset):\n",
    "    def __init__(self, dataframe, mode, transforms=[]):\n",
    "        self.data = dataframe[dataframe.set == mode]\n",
    "        self.transforms = transforms\n",
    "        self.mode = mode\n",
    "        \n",
    "    def __len__(self):\n",
    "        return 2*len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        _idx = []\n",
    "        \n",
    "        if torch.is_tensor(idx):\n",
    "            _idx = idx.tolist()\n",
    "        \n",
    "        if isinstance(idx, list):\n",
    "            _idx = idx\n",
    "        else:\n",
    "            _idx = [ idx ]\n",
    "        \n",
    "        _ids = [ (i - (i % 2))/2 for i in _idx]\n",
    "\n",
    "        data = self.data.iloc[_ids, :]\n",
    "        data['asked_id'] = _idx\n",
    "        \n",
    "        data = pd.DataFrame(\n",
    "            {\n",
    "                'set': [self.mode for _ in range(0, len(_ids))],\n",
    "                'mode': [ (0 if i % 2 == 0 else 1) for i in _idx ],\n",
    "                'text': data.apply(lambda row: row['text'] if row['asked_id'] % 2 == 0 else row['headline'], axis=1),\n",
    "                'title': data['normalized_title']\n",
    "            }\n",
    "        )\n",
    "\n",
    "        for transform in self.transforms:\n",
    "            data = transform(data)\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextToParsedDoc(object):\n",
    "    def __init__(self, nlp):\n",
    "        self.nlp = nlp\n",
    "        \n",
    "    def __call__(self, sample):\n",
    "        sample['doc'] = sample.apply(lambda row: self.nlp(row['text']), axis=1)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordsToVectors(object):\n",
    "    def __init__(self, nlp):\n",
    "        self.nlp = nlp\n",
    "        \n",
    "    def document_embeddings(self, doc):\n",
    "        word_embeddings = [\n",
    "            [ l.vector ] if l.whitespace_ == '' else [ l.vector, np.zeros_like(l.vector) ] for l in doc\n",
    "        ]\n",
    "\n",
    "        return np.stack(\n",
    "            [\n",
    "                vector for vectors in word_embeddings for vector in vectors\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        \n",
    "        sample['word_embeddings'] = sample.apply(\n",
    "            lambda row: self.document_embeddings(row['doc']),\n",
    "            axis=1\n",
    "        )\n",
    "    \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddNoiseToEmbeddings(object):\n",
    "    def __init__(self, probability_of_mask_for_word):\n",
    "        self.probability_of_mask_for_word = probability_of_mask_for_word\n",
    "        self.rng = np.random.default_rng()\n",
    "        \n",
    "    def mask_vector(self, vector):\n",
    "        \"\"\"\n",
    "        Masks words with zeros randomly\n",
    "        \"\"\"\n",
    "        seq_len = vector.shape[0]\n",
    "        vector_len = vector.shape[1]\n",
    "        \n",
    "        mask = np.repeat(\n",
    "            self.rng.choice(\n",
    "                [0, 1],\n",
    "                seq_len,\n",
    "                p=[\n",
    "                    self.probability_of_mask_for_word,\n",
    "                    (1 - self.probability_of_mask_for_word)\n",
    "                ]\n",
    "            ).reshape((seq_len, 1)),\n",
    "            vector_len,\n",
    "            axis=1\n",
    "        )\n",
    "        \n",
    "        return vector * mask\n",
    "        \n",
    "    def __call__(self, sample):       \n",
    "        sample['noisy_word_embeddings'] = sample['word_embeddings'].apply(self.mask_vector)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MergeBatch(object):\n",
    "    def stack_vectors(self, vectors):\n",
    "        max_seq = max([vector.shape[0] for vector in vectors])\n",
    "        \n",
    "        return np.stack(\n",
    "            [\n",
    "                np.pad(vector, [(0, max_seq - vector.shape[0]), (0, 0)])\n",
    "                for vector in vectors\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "    def __call__(self, sample):\n",
    "        del sample['doc']\n",
    "        \n",
    "        sample = sample.to_dict(orient=\"list\")\n",
    "        \n",
    "        sample['word_embeddings'] = self.stack_vectors(sample['word_embeddings'])\n",
    "        sample['noisy_word_embeddings'] = self.stack_vectors(sample['noisy_word_embeddings'])\n",
    "    \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader(object):\n",
    "    def __init__(self, dataset, batch_size=8, num_workers=1):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        \n",
    "    @property\n",
    "    def epoch_size(self):\n",
    "        return math.ceil(len(self.dataset) / self.batch_size) * self.batch_size\n",
    "    \n",
    "    def __iter__(self):\n",
    "        ids = random.choices(range(0, len(self.dataset)), k=self.epoch_size)\n",
    "        \n",
    "        for start_ix in range(0, self.epoch_size, self.batch_size):\n",
    "            yield self.dataset[ids[start_ix:(start_ix + self.batch_size)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArticlesBatch:\n",
    "    def __init__(self, data, id):\n",
    "        self.data = data\n",
    "        self.id = id\n",
    "        \n",
    "    def __getattr__(self, name):\n",
    "        if name in self.data:\n",
    "            return self.data[name]\n",
    "        else:\n",
    "            raise AttributeError(f\"Attribute missing: {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(object):\n",
    "    def __init__(self, nlp):\n",
    "        self.nlp = nlp\n",
    "        \n",
    "    def decode_embeddings(self, word_embeddings):\n",
    "        \"\"\"\n",
    "        Decodes a single document. Word embeddings given are of shape (N, D)\n",
    "        where N is the number of lexemes and D the dimentionality of the embedding vector\n",
    "        \"\"\"\n",
    "        \n",
    "        return \"\".join(\n",
    "            [\n",
    "                token.text.lower() if not token.is_oov else \" \"\n",
    "                for token in [\n",
    "                    self.nlp.vocab[ks[0]]\n",
    "                    for ks in self.nlp.vocab.vectors.most_similar(\n",
    "                        word_embeddings, n=1\n",
    "                    )[0]\n",
    "                ]\n",
    "            ]\n",
    "        ).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpdateInfo(object):\n",
    "    def __init__(self, decoder, batch, word_embeddings, loss_sum, mode):\n",
    "        self.decoder = decoder\n",
    "        self.batch = batch\n",
    "        self.word_embeddings = word_embeddings\n",
    "        self.loss_sum = loss_sum\n",
    "        self.mode = mode\n",
    "        \n",
    "    @classmethod\n",
    "    def empty(cls, nlp, mode):\n",
    "        return UpdateInfo(nlp, None, None, None, mode)\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"{self.mode} | {self.batch.id}\\t| Loss: {loss_sum}\\t\"\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseTrainer:\n",
    "    def __init__(self, name, nlp, dataframe,\n",
    "                 optimizer_class_name,\n",
    "                 model_args, optimizer_args, \n",
    "                 batch_size, update_every, loader_workers,\n",
    "                 probability_of_mask_for_word\n",
    "                ):\n",
    "        self.name = name\n",
    "        \n",
    "        self.datasets = {\n",
    "            \"train\": ArticlesDataset(\n",
    "                dataframe,\n",
    "                \"train\",\n",
    "                transforms=[\n",
    "                    TextToParsedDoc(nlp),\n",
    "                    WordsToVectors(nlp),\n",
    "                    AddNoiseToEmbeddings(probability_of_mask_for_word),\n",
    "                    MergeBatch()\n",
    "                ]\n",
    "            ),\n",
    "            \"test\":  ArticlesDataset(\n",
    "                dataframe,\n",
    "                \"test\",\n",
    "                transforms=[\n",
    "                    TextToParsedDoc(nlp),\n",
    "                    WordsToVectors(nlp),\n",
    "                    AddNoiseToEmbeddings(0),\n",
    "                    MergeBatch()\n",
    "                ]\n",
    "            ),\n",
    "            \"eval\":  ArticlesDataset(\n",
    "                dataframe,\n",
    "                \"eval\",\n",
    "                transforms=[\n",
    "                    TextToParsedDoc(nlp),\n",
    "                    WordsToVectors(nlp),\n",
    "                    AddNoiseToEmbeddings(0),\n",
    "                    MergeBatch()\n",
    "                ]\n",
    "            )\n",
    "        }\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.update_every = update_every\n",
    "        self.loader_workers = loader_workers\n",
    "        \n",
    "        self.optimizer_class_name = optimizer_class_name\n",
    "        \n",
    "        self.model_args = model_args\n",
    "        self.optimizer_args = optimizer_args\n",
    "        \n",
    "        self.current_batch_id = 0\n",
    "        \n",
    "        self.decoder = Decoder(nlp)\n",
    "        \n",
    "        if self.has_checkpoint:\n",
    "            self.load_last_checkpoint()\n",
    "        \n",
    "    @cached_property\n",
    "    def model(self):\n",
    "        try:\n",
    "            return SummarizeNet.load(f\"{self.checkpoint_path}/model.pth\")\n",
    "        except FileNotFoundError:\n",
    "            return SummarizeNet(self.model_args)\n",
    "        \n",
    "    @cached_property\n",
    "    def optimizer(self):\n",
    "        class_ = getattr(torch.optim, self.optimizer_class_name)\n",
    "        \n",
    "        return class_(self.model.parameters(), **self.optimizer_args)\n",
    "    \n",
    "    @property\n",
    "    def checkpoint_path(self):\n",
    "        return f\"checkpoints/{self.name}/batch-#{self.current_batch_id}\"\n",
    "    \n",
    "    def save_checkpoint(self):\n",
    "        os.makedirs(self.checkpoint_path, exist_ok=True)\n",
    "        \n",
    "        self.model.save(f\"{self.checkpoint_path}/model.pth\")\n",
    "        \n",
    "        torch.save(\n",
    "            {\n",
    "                'current_batch_id': self.current_batch_id,\n",
    "                'batch_size': self.batch_size,\n",
    "                'update_every': self.update_every,\n",
    "                'optimizer_class_name': self.optimizer_class_name,\n",
    "                'optimizer_args': self.optimizer_args,\n",
    "                'optimizer_state_dict': self.optimizer.state_dict()\n",
    "            },\n",
    "            f\"{self.checkpoint_path}/trainer.pth\"\n",
    "        )\n",
    "    \n",
    "    @property\n",
    "    def checkpoint_directories(self):\n",
    "        return sorted(f\"checkpoints/{self.name}/batch-*\", reverse=True)\n",
    "    \n",
    "    @property\n",
    "    def has_checkpoint():\n",
    "        return len(self.checkpoint_directories) > 0\n",
    "    \n",
    "    def load_last_checkpoint():\n",
    "        path = next(self.checkpoint_directories)\n",
    "        \n",
    "        data = torch.load(f\"{path}/trainer.pth\")\n",
    "        \n",
    "        self.batch_size = data['current_batch_id']\n",
    "        self.update_every = data['update_every']\n",
    "        self.loader_workers = data['loader_workers']\n",
    "        \n",
    "        self.optimizer_class_name = data['optimizer_class_name']\n",
    "        self.optimizer_args = data['optimizer_args']\n",
    "        \n",
    "        self.current_batch_id = data['current_batch_id']\n",
    "        \n",
    "        del self.__dict__['model']\n",
    "        def self.__dict__['optimizer']\n",
    "        \n",
    "        self.optimizer.load_state_dict(data['optimizer_state_dict'])\n",
    "    \n",
    "    def batches(self, mode):\n",
    "        start_id = self.current_batch_id\n",
    "        \n",
    "        while True:\n",
    "            loader = DataLoader(\n",
    "                self.datasets[mode],\n",
    "                batch_size=self.batch_size,\n",
    "                num_workers=self.loader_workers\n",
    "            )\n",
    "\n",
    "            for ix, data in enumerate(loader):\n",
    "                self.current_batch_id += ix\n",
    "                \n",
    "                yield(\n",
    "                    ArticlesBatch(\n",
    "                        data,\n",
    "                        id=self.current_batch_id\n",
    "                    )\n",
    "                )\n",
    "    \n",
    "    def work_batch(self, batch):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def updates(self, mode=\"train\", update_every=None):\n",
    "        batches = self.batches(mode)\n",
    "        loss_sum = 0\n",
    "        \n",
    "        if update_every is None:\n",
    "            update_every = self.update_every\n",
    "        \n",
    "        for batch in batches:\n",
    "            if mode == \"train\":\n",
    "                self.model.train()\n",
    "            else:\n",
    "                self.model.eval()\n",
    "            \n",
    "            loss, word_embeddings = self.work_batch(batch)\n",
    "            loss /= self.update_every * self.batch_size\n",
    "            \n",
    "            loss.backward()\n",
    "            loss_sum += loss\n",
    "            \n",
    "            # we're doing the accumulated gradients trick to get the gradients variance\n",
    "            # down while being able to use commodity GPU:\n",
    "            if batch.id % update_every == 0:\n",
    "                if mode == \"train\":\n",
    "                    self.optimizer.step()\n",
    "                    self.optimizer.zero_grad()\n",
    "                \n",
    "                yield(UpdateInfo(self.decoder, batch, word_embeddings, loss_sum, mode=mode))\n",
    "                \n",
    "                loss_sum = 0\n",
    "    \n",
    "    def train_and_evaluate_updates(self, evaluate_every=100):\n",
    "        train_updates = self.updates(mode=\"train\")\n",
    "        evaluate_updates = self.updates(mode=\"eval\")\n",
    "        \n",
    "        for update_info in train_updates:\n",
    "            yield(update_info)\n",
    "            \n",
    "            if update_info.batch.id % evaluate_every == 0:\n",
    "                yield(next(evaluate_updates))\n",
    "    \n",
    "    def test_updates(self):\n",
    "        return self.updates(mode=\"test\", update_every=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(BaseTrainer):\n",
    "    def __init__(*args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "    def compute_loss(self, word_embeddings, original_word_embeddings, discriminate_probs):\n",
    "        embeddings_loss = F.cosine_embedding_loss(\n",
    "            word_embeddings,\n",
    "            orig_word_embeddings,\n",
    "            torch.ones(word_embeddings.shape[0])\n",
    "        )\n",
    "        \n",
    "        discriminator_loss = F.binary_cross_entropy(\n",
    "            discriminate_probs,\n",
    "            torch.zeros_like(discriminate_probs)\n",
    "        )\n",
    "        \n",
    "        return embeddings_loss + discriminator_loss\n",
    "        \n",
    "\n",
    "    def work_batch(self, batch):\n",
    "        word_embeddings, discriminate_probs = self.summarize_model(\n",
    "            batch.noisy_word_embeddings\n",
    "        )\n",
    "\n",
    "        # we're diverging from the article here by outputting the word embeddings\n",
    "        # instead of the probabilities for each word in a vocabulary\n",
    "        # our loss function is using the cosine embedding loss coupled with\n",
    "        # the discriminator loss:\n",
    "        return (\n",
    "            self.compute_loss(word_embeddings, batch.word_embeddings, discriminate_probs),\n",
    "            word_embeddings\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InNotebookTrainer(Trainer):\n",
    "    def __init__(*args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def train(self):\n",
    "        cumulative_train_info = UpdateInfo.empty(self.decoder, mode=\"train\")\n",
    "        cumulative_evaluate_info = UpdateInfo.empty(self.decoder, mode=\"eval\")\n",
    "\n",
    "        for update_info in trainer.train_and_evaluate_updates():\n",
    "            if update_info.from_train:\n",
    "                cumulative_train_info += update_info\n",
    "\n",
    "                print(f\"{cumulative_train_info}\")\n",
    "\n",
    "            if update_info.from_evaluate:\n",
    "                cumulative_evaluate_info += update_info\n",
    "\n",
    "                print(f\"{cumulative_evaluate_info}\")\n",
    "\n",
    "                trainer.save_checkpoint()\n",
    "                \n",
    "    def test(self):\n",
    "        cumulative_info = UpdateInfo.empty(mode=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(145, 300)\n",
      "CPU times: user 4.4 s, sys: 1.78 s, total: 6.18 s\n",
      "Wall time: 1.37 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'the recommended way to upgrade from ubuntu 14.04 lts is to first upgrade to 16.04 lts, then to 18.04 lts, which will continue to receive support until april 2023. ubuntu has lts -> lts upgrades, allowing you to skip intermediate non-lts releases, but we can’t skip intermediate lts releases; we have to go via 16.04, unless we want to do a fresh install of 18.04 lts.'"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "document = \"\"\"\n",
    "The recommended way to upgrade from Ubuntu 14.04 LTS is to first upgrade to 16.04 LTS, then to 18.04 LTS, which will continue to receive support until April 2023. Ubuntu has LTS -> LTS upgrades, allowing you to skip intermediate non-LTS releases, but we can’t skip intermediate LTS releases; we have to go via 16.04, unless we want to do a fresh install of 18.04 LTS.\n",
    "\"\"\"\n",
    "\n",
    "word_embeddings = [\n",
    "    [ t.vector ] if t.whitespace_ == '' else [ t.vector, np.zeros_like(t.vector) ] for t in nlp(document)\n",
    "]\n",
    "\n",
    "word_embeddings = np.stack(\n",
    "    [\n",
    "        vector for vectors in word_embeddings for vector in vectors\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(word_embeddings.shape)\n",
    "\n",
    "\"\".join(\n",
    "    [\n",
    "        token.text.lower() if not token.is_oov else \" \"\n",
    "        for token in [\n",
    "            nlp.vocab[ks[0]]\n",
    "            for ks in nlp.vocab.vectors.most_similar(\n",
    "                word_embeddings, n=1\n",
    "            )[0]\n",
    "        ]\n",
    "    ]\n",
    ").strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.087595 ,  0.35502  ,  0.063868 , ...,  0.03446  , -0.15027  ,\n",
       "         0.40673  ],\n",
       "       [-0.084961 ,  0.502    ,  0.0023823, ..., -0.21511  , -0.26304  ,\n",
       "        -0.0060173],\n",
       "       [-0.025563 ,  0.44424  , -0.24555  , ..., -0.029137 ,  0.062257 ,\n",
       "         0.090782 ],\n",
       "       ...,\n",
       "       [-0.084961 ,  0.502    ,  0.0023823, ..., -0.21511  , -0.26304  ,\n",
       "        -0.0060173],\n",
       "       [-0.062456 ,  0.026028 , -0.2255   , ..., -0.19075  , -0.26296  ,\n",
       "         0.32319  ],\n",
       "       [-0.23011  ,  0.24952  , -0.40514  , ...,  0.049255 , -0.22886  ,\n",
       "        -0.23064  ]], dtype=float32)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.stack([ t.vector for t in nlp(\"This is just a test sentence. This is another sentence\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ones((1, 300)).all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
