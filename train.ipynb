{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import functools\n",
    "import statistics\n",
    "import itertools\n",
    "import random\n",
    "import math\n",
    "from pathlib import Path\n",
    "import pdb\n",
    "\n",
    "import pandas as pd\n",
    "import swifter\n",
    "import numpy as np\n",
    "import hickle as hkl\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import spacy\n",
    "from cached_property import cached_property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'nlp' not in vars():\n",
    "    nlp = spacy.load(\n",
    "        \"en_core_web_lg\",\n",
    "        disable=[\"tagger\", \"ner\", \"textcat\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'articles' not in vars():\n",
    "    articles = pd.read_parquet(\"data/articles-processed.parquet.gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNModel(nn.Module):\n",
    "    def __init__(self, *_args, **_kwargs):\n",
    "        super().__init__()\n",
    "        \n",
    "        self._args = _args\n",
    "        self._kwargs = _kwargs\n",
    "        \n",
    "    def save(self, path):\n",
    "        torch.save(\n",
    "            {\n",
    "                'state': self.state_dict(),\n",
    "                'args': self._args,\n",
    "                'kwargs': self._kwargs\n",
    "            },\n",
    "            path\n",
    "        )\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, path):\n",
    "        if Path(path).exists():\n",
    "            data = torch.load(path)\n",
    "            \n",
    "            model = cls(*data['args'], **data['kwargs'])\n",
    "            model.load_state_dict(data['state'])\n",
    "\n",
    "            return model\n",
    "        else:\n",
    "            raise FileNotFoundError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscriminatorNet(NNModel):\n",
    "    def __init__(self, input_size):\n",
    "        super(DiscriminatorNet, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        \n",
    "        self.linear = nn.Linear(input_size, 1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"\n",
    "        The forward pass for the network\n",
    "        \n",
    "        hidden_state : tensor (batch_num, hidden_size)\n",
    "        \n",
    "        returns         : tensor (batch_num, 1)\n",
    "        \"\"\"\n",
    "        \n",
    "        state = state.transpose(0, 1).reshape(-1, self.input_size)\n",
    "        state = self.linear(state)\n",
    "        state = F.sigmoid(state)\n",
    "        \n",
    "        return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SummarizeNet(NNModel):\n",
    "    def __init__(self, hidden_size, input_size, num_layers):\n",
    "        super(SummarizeNet, self).__init__(\n",
    "            hidden_size=hidden_size,\n",
    "            input_size=input_size,\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.encode_gru = nn.GRU(\n",
    "            input_size,\n",
    "            hidden_size,\n",
    "            num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        \n",
    "        self.decode_gru = nn.GRU(\n",
    "            hidden_size,\n",
    "            input_size,\n",
    "            num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        \n",
    "        self.discriminate = DiscriminatorNet(num_layers * 2 * input_size)\n",
    "        \n",
    "    def take_last_pass(self, predicted):\n",
    "        return predicted.reshape(\n",
    "            predicted.shape[0],\n",
    "            predicted.shape[1],\n",
    "            2,\n",
    "            int(predicted.shape[2] / 2)\n",
    "        )[:, :, 1, :]\n",
    "\n",
    "    def forward(self, word_embeddings, modes):\n",
    "        \"\"\"\n",
    "        The forward pass for the network\n",
    "        \n",
    "        word_embeddings : tensor (batch_num, max_seq_len, embedding_length)\n",
    "        \n",
    "        returns         : tuple (\n",
    "                            tensor (batch_num, max_seq_len, vocab_len),\n",
    "                            tensor (batch_num, hidden_size)\n",
    "                          )\n",
    "        \n",
    "        First tensor in the returning tuple are predicted word embeddings\n",
    "        The second tensor are probabilities of the output being a headline\n",
    "        \"\"\"\n",
    "        \n",
    "        predicted, _ = self.encode_gru(word_embeddings)\n",
    "        predicted = self.take_last_pass(predicted)\n",
    "        \n",
    "        predicted, state = self.decode_gru(predicted)\n",
    "        predicted = self.take_last_pass(predicted)\n",
    "        \n",
    "        predicted_modes = self.discriminate(state)\n",
    "        \n",
    "        return predicted, predicted_modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArticlesDataset(Dataset):\n",
    "    def __init__(self, dataframe, mode, transforms=[]):\n",
    "        if mode not in ['train', 'test', 'val']:\n",
    "            raise ValueError(f\"{mode} not in the set of modes of the dataset (['train', 'test', 'val'])\")\n",
    "            \n",
    "        self.data = dataframe[dataframe.set == mode]\n",
    "        self.transforms = transforms\n",
    "        self.mode = mode\n",
    "        \n",
    "    def __len__(self):\n",
    "        return 2*len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        _idx = []\n",
    "        \n",
    "        if torch.is_tensor(idx):\n",
    "            _idx = idx.tolist()\n",
    "        \n",
    "        if isinstance(idx, list):\n",
    "            _idx = idx\n",
    "        else:\n",
    "            _idx = [ idx ]\n",
    "        \n",
    "        _ids = [ (i - (i % 2))/2 for i in _idx]\n",
    "\n",
    "        data = self.data.iloc[_ids, :]\n",
    "        data['asked_id'] = _idx\n",
    "        \n",
    "        data = pd.DataFrame(\n",
    "            {\n",
    "                'set': [self.mode for _ in range(0, len(_ids))],\n",
    "                'mode': np.array([ (0.0 if i % 2 == 0 else 1.0) for i in _idx ]),\n",
    "                'text': data.apply(lambda row: row['text'] if row['asked_id'] % 2 == 0 else row['headline'], axis=1),\n",
    "                'title': data['normalized_title']\n",
    "            }\n",
    "        )\n",
    "\n",
    "        for transform in self.transforms:\n",
    "            data = transform(data)\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextToParsedDoc(object):\n",
    "    def __init__(self, nlp):\n",
    "        self.nlp = nlp\n",
    "        \n",
    "    def __call__(self, sample):\n",
    "        sample['doc'] = sample.swifter.progress_bar(False).apply(lambda row: self.nlp(row['text']), axis=1)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordsToVectors(object):\n",
    "    def __init__(self, nlp):\n",
    "        self.nlp = nlp\n",
    "        \n",
    "    def document_embeddings(self, doc):\n",
    "        word_embeddings = [\n",
    "            [ l.vector ] if l.whitespace_ == '' else [ l.vector, np.zeros_like(l.vector) ] for l in doc\n",
    "        ]\n",
    "\n",
    "        return np.stack(\n",
    "            [\n",
    "                vector for vectors in word_embeddings for vector in vectors\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        \n",
    "        sample['word_embeddings'] = sample.swifter.progress_bar(False).apply(\n",
    "            lambda row: self.document_embeddings(row['doc']),\n",
    "            axis=1\n",
    "        )\n",
    "    \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddNoiseToEmbeddings(object):\n",
    "    def __init__(self, probability_of_mask_for_word):\n",
    "        self.probability_of_mask_for_word = probability_of_mask_for_word\n",
    "        self.rng = np.random.default_rng()\n",
    "        \n",
    "    def mask_vector(self, vector):\n",
    "        \"\"\"\n",
    "        Masks words with zeros randomly\n",
    "        \"\"\"\n",
    "        seq_len = vector.shape[0]\n",
    "        vector_len = vector.shape[1]\n",
    "        \n",
    "        mask = np.repeat(\n",
    "            self.rng.choice(\n",
    "                [0, 1],\n",
    "                seq_len,\n",
    "                p=[\n",
    "                    self.probability_of_mask_for_word,\n",
    "                    (1 - self.probability_of_mask_for_word)\n",
    "                ]\n",
    "            ).reshape((seq_len, 1)),\n",
    "            vector_len,\n",
    "            axis=1\n",
    "        )\n",
    "        \n",
    "        return vector * mask\n",
    "        \n",
    "    def __call__(self, sample):       \n",
    "        sample['noisy_word_embeddings'] = sample['word_embeddings'].apply(self.mask_vector)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MergeBatch(object):\n",
    "    def __init__(self, device):\n",
    "        self.device = device\n",
    "        \n",
    "    def stack_vectors(self, vectors):\n",
    "        max_seq = max([vector.shape[0] for vector in vectors])\n",
    "        \n",
    "        return np.stack(\n",
    "            [\n",
    "                np.pad(vector, [(0, max_seq - vector.shape[0]), (0, 0)])\n",
    "                for vector in vectors\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "    def __call__(self, sample):\n",
    "        del sample['doc']\n",
    "        \n",
    "        sample = sample.to_dict(orient=\"list\")\n",
    "        \n",
    "        sample['word_embeddings'] = torch.from_numpy(\n",
    "            self.stack_vectors(\n",
    "                sample['word_embeddings']\n",
    "            ).astype(np.float32, copy=False)\n",
    "        ).to(self.device)\n",
    "        \n",
    "        if 'noisy_word_embeddings' in sample:\n",
    "            sample['noisy_word_embeddings'] = torch.from_numpy(\n",
    "                self.stack_vectors(\n",
    "                    sample['noisy_word_embeddings']\n",
    "                ).astype(np.float32, copy=False)\n",
    "            ).to(self.device)\n",
    "        \n",
    "        sample['mode'] = torch.from_numpy(\n",
    "            np.stack(\n",
    "                sample['mode']\n",
    "            ).astype(np.float32, copy=False)\n",
    "        ).to(self.device)\n",
    "    \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SetAllToSummarizing(object):\n",
    "    def __call__(self, sample):\n",
    "        sample['mode'] = np.ones_like(sample['mode']).astype(np.float32, copy=False)\n",
    "        \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    def __init__(self, nlp, series):\n",
    "        if Path(\"vocabulary.hkl\").exists():\n",
    "            data = hkl.load(\"vocabulary.hkl\")\n",
    "            \n",
    "            self.words = data['words']\n",
    "            self.index = data['index']\n",
    "        else:\n",
    "            text = \"\"\n",
    "            words = []\n",
    "            index = {}\n",
    "            \n",
    "            for serie in series:\n",
    "                text += \" \" + \" \".join(serie.fillna('').values.tolist())\n",
    "                \n",
    "            counts = nlp(text).count_by(spacy.attrs.LOWER)\n",
    "\n",
    "            for ix, _ in sorted([(ix, counts[ix]) for ix in counts],key=lambda t: t[1],reverse=True):\n",
    "                words.append(nlp.vocab[ix].text)\n",
    "                index[ix] = len(words)\n",
    "                \n",
    "            self.words = words\n",
    "            self.index = index\n",
    "            \n",
    "            hkl.dump({words: words, index: index}, 'vocabulary.hkl', mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader(object):\n",
    "    def __init__(self, dataset, batch_size=8):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    @property\n",
    "    def epoch_size(self):\n",
    "        return math.ceil(len(self.dataset) / self.batch_size) * self.batch_size\n",
    "    \n",
    "    def __iter__(self):\n",
    "        ids = random.choices(range(0, len(self.dataset)), k=self.epoch_size)\n",
    "        \n",
    "        for start_ix in range(0, self.epoch_size, self.batch_size):\n",
    "            yield self.dataset[ids[start_ix:(start_ix + self.batch_size)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArticlesBatch:\n",
    "    def __init__(self, data, ix=0):\n",
    "        self.data = data\n",
    "        self.ix = ix\n",
    "    \n",
    "        \n",
    "    def __getattr__(self, name):\n",
    "        if name in self.data:\n",
    "            return self.data[name]\n",
    "        else:\n",
    "            raise AttributeError(f\"Attribute missing: {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(object):\n",
    "    def __init__(self, nlp):\n",
    "        self.nlp = nlp\n",
    "\n",
    "    def decode_embeddings(self, word_embeddings):\n",
    "        data = word_embeddings.cpu().data.numpy()\n",
    "        \n",
    "        return [\n",
    "            self.decode_embeddings_1d(data[ix, :, :])\n",
    "            for ix in range(0, data.shape[0])\n",
    "        ]\n",
    "        \n",
    "    def decode_embeddings_1d(self, word_embeddings):\n",
    "        \"\"\"\n",
    "        Decodes a single document. Word embeddings given are of shape (N, D)\n",
    "        where N is the number of lexemes and D the dimentionality of the embedding vector\n",
    "        \"\"\"\n",
    "        \n",
    "        return \"\".join(\n",
    "            [\n",
    "                token.text.lower() if not token.is_oov else \" \"\n",
    "                for token in [\n",
    "                    self.nlp.vocab[ks[0]]\n",
    "                    for ks in self.nlp.vocab.vectors.most_similar(\n",
    "                        word_embeddings, n=1\n",
    "                    )[0]\n",
    "                ]\n",
    "            ]\n",
    "        ).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metrics(object):\n",
    "    def __init__(self, mode, loss=None):\n",
    "        self.mode = mode\n",
    "        self.losses = [loss.cpu().item()] if loss is not None else []\n",
    "    \n",
    "    @classmethod\n",
    "    def empty(cls, mode):\n",
    "        return cls(mode)\n",
    "    \n",
    "    @property\n",
    "    def loss(self):\n",
    "        if len(self.losses) == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return statistics.mean(self.losses)\n",
    "    \n",
    "    @property\n",
    "    def last_loss(self):\n",
    "        return self.losses[len(self.losses) - 1]\n",
    "    \n",
    "    def running_mean_loss(self, n=100):\n",
    "        cumsum = np.cumsum(np.insert(np.array(self.losses), 0, 0)) \n",
    "        return (cumsum[n:] - cumsum[:-n]) / float(n)\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        self.losses += other.losses\n",
    "        \n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpdateInfo(object):\n",
    "    def __init__(self, decoder, batch, word_embeddings, loss_sum, mode):\n",
    "        self.decoder = decoder\n",
    "        self.batch = batch\n",
    "        self.word_embeddings = word_embeddings\n",
    "        self.loss_sum = loss_sum\n",
    "        self.mode = mode\n",
    "        \n",
    "    @property\n",
    "    def from_train(self):\n",
    "        return self.mode == \"train\"\n",
    "    \n",
    "    @property\n",
    "    def from_evaluate(self):\n",
    "        return self.mode == \"val\"\n",
    "        \n",
    "    @cached_property\n",
    "    def decoded_inferred_texts(self):\n",
    "        return self.decoder.decode_embeddings(self.word_embeddings)\n",
    "    \n",
    "    @cached_property\n",
    "    def metrics(self):\n",
    "        return Metrics(self.mode, self.loss_sum)\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"{self.mode} | {self.batch.ix}\\t| Loss: {loss_sum}\\t\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseTrainer:\n",
    "    def __init__(self, name, nlp, dataframe,\n",
    "                 optimizer_class_name,\n",
    "                 model_args, optimizer_args, \n",
    "                 batch_size, update_every,\n",
    "                 probability_of_mask_for_word,\n",
    "                 device\n",
    "                ):\n",
    "        self.name = name\n",
    "        \n",
    "        self.device = device\n",
    "        \n",
    "        self.datasets = {\n",
    "            \"train\": ArticlesDataset(\n",
    "                dataframe,\n",
    "                \"train\",\n",
    "                transforms=[\n",
    "                    TextToParsedDoc(nlp),\n",
    "                    WordsToVectors(nlp),\n",
    "                    AddNoiseToEmbeddings(probability_of_mask_for_word),\n",
    "                    MergeBatch(device)\n",
    "                ]\n",
    "            ),\n",
    "            \"test\":  ArticlesDataset(\n",
    "                dataframe,\n",
    "                \"test\",\n",
    "                transforms=[\n",
    "                    TextToParsedDoc(nlp),\n",
    "                    WordsToVectors(nlp),\n",
    "                    AddNoiseToEmbeddings(0),\n",
    "                    SetAllToSummarizing(),\n",
    "                    MergeBatch(device)\n",
    "                ]\n",
    "            ),\n",
    "            \"val\":  ArticlesDataset(\n",
    "                dataframe,\n",
    "                \"val\",\n",
    "                transforms=[\n",
    "                    TextToParsedDoc(nlp),\n",
    "                    WordsToVectors(nlp),\n",
    "                    AddNoiseToEmbeddings(0),\n",
    "                    MergeBatch(device)\n",
    "                ]\n",
    "            )\n",
    "        }\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.update_every = update_every\n",
    "        \n",
    "        self.optimizer_class_name = optimizer_class_name\n",
    "        \n",
    "        self.model_args = model_args\n",
    "        self.optimizer_args = optimizer_args\n",
    "        \n",
    "        self.current_batch_id = 0\n",
    "        \n",
    "        self.decoder = Decoder(nlp)\n",
    "        \n",
    "        if self.has_checkpoint:\n",
    "            self.load_last_checkpoint()\n",
    "        \n",
    "    @cached_property\n",
    "    def model(self):\n",
    "        try:\n",
    "            return SummarizeNet.load(f\"{self.checkpoint_path}/model.pth\").to(self.device)\n",
    "        except FileNotFoundError:\n",
    "            return SummarizeNet(**self.model_args).to(self.device)\n",
    "        \n",
    "    @cached_property\n",
    "    def optimizer(self):\n",
    "        class_ = getattr(torch.optim, self.optimizer_class_name)\n",
    "        \n",
    "        return class_(self.model.parameters(), **self.optimizer_args)\n",
    "    \n",
    "    @property\n",
    "    def checkpoint_path(self):\n",
    "        return f\"checkpoints/{self.name}/batch-#{self.current_batch_id}\"\n",
    "    \n",
    "    def save_checkpoint(self):\n",
    "        os.makedirs(self.checkpoint_path, exist_ok=True)\n",
    "        \n",
    "        self.model.save(f\"{self.checkpoint_path}/model.pth\")\n",
    "        \n",
    "        torch.save(\n",
    "            {\n",
    "                'current_batch_id': self.current_batch_id,\n",
    "                'batch_size': self.batch_size,\n",
    "                'update_every': self.update_every,\n",
    "                'optimizer_class_name': self.optimizer_class_name,\n",
    "                'optimizer_args': self.optimizer_args,\n",
    "                'optimizer_state_dict': self.optimizer.state_dict()\n",
    "            },\n",
    "            f\"{self.checkpoint_path}/trainer.pth\"\n",
    "        )\n",
    "    \n",
    "    @property\n",
    "    def checkpoint_directories(self):\n",
    "        return sorted(Path(\".\").glob(f\"checkpoints/{self.name}/batch-*\"), reverse=True)\n",
    "    \n",
    "    @property\n",
    "    def has_checkpoint(self):\n",
    "        return len(self.checkpoint_directories) > 0\n",
    "    \n",
    "    def load_last_checkpoint(self):\n",
    "        path = self.checkpoint_directories[0]\n",
    "        \n",
    "        data = torch.load(f\"{path}/trainer.pth\")\n",
    "        \n",
    "        self.batch_size = data['batch_size']\n",
    "        self.update_every = data['update_every']\n",
    "        \n",
    "        self.optimizer_class_name = data['optimizer_class_name']\n",
    "        self.optimizer_args = data['optimizer_args']\n",
    "        \n",
    "        self.current_batch_id = data['current_batch_id']\n",
    "        \n",
    "        if 'model' in self.__dict__:\n",
    "            del self.__dict__['model']\n",
    "            \n",
    "        if 'optimzer' in self.__dict__:\n",
    "            del self.__dict__['optimizer']\n",
    "        \n",
    "        self.optimizer.load_state_dict(data['optimizer_state_dict'])\n",
    "    \n",
    "    def batches(self, mode):       \n",
    "        while True:\n",
    "            loader = DataLoader(\n",
    "                self.datasets[mode],\n",
    "                batch_size=self.batch_size\n",
    "            )\n",
    "\n",
    "            for data in loader:\n",
    "                self.current_batch_id += 1\n",
    "                \n",
    "                yield(\n",
    "                    ArticlesBatch(\n",
    "                        data,\n",
    "                        ix=self.current_batch_id\n",
    "                    )\n",
    "                )\n",
    "    \n",
    "    def work_batch(self, batch):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def updates(self, mode=\"train\", update_every=None):\n",
    "        batches = self.batches(mode)\n",
    "        loss_sum = 0\n",
    "        \n",
    "        if update_every is None:\n",
    "            update_every = self.update_every\n",
    "        \n",
    "        for batch in batches:\n",
    "            if mode == \"train\":\n",
    "                self.model.train()\n",
    "            else:\n",
    "                self.model.eval()\n",
    "            \n",
    "            loss, word_embeddings = self.work_batch(batch)\n",
    "            loss /= self.update_every\n",
    "            \n",
    "            if mode == \"train\":\n",
    "                loss.backward()\n",
    "                \n",
    "            loss_sum += loss\n",
    "            \n",
    "            # we're doing the accumulated gradients trick to get the gradients variance\n",
    "            # down while being able to use commodity GPU:\n",
    "            if batch.ix % update_every == 0:\n",
    "                if mode == \"train\":\n",
    "                    self.optimizer.step()\n",
    "                    self.optimizer.zero_grad()\n",
    "                \n",
    "                yield(UpdateInfo(self.decoder, batch, word_embeddings, loss_sum, mode=mode))\n",
    "                \n",
    "                loss_sum = 0\n",
    "    \n",
    "    def train_and_evaluate_updates(self, evaluate_every=100):\n",
    "        train_updates = self.updates(mode=\"train\")\n",
    "        evaluate_updates = self.updates(mode=\"val\")\n",
    "        \n",
    "        for update_info in train_updates:\n",
    "            yield(update_info)\n",
    "            \n",
    "            if update_info.batch.ix != 0 and update_info.batch.ix % evaluate_every == 0:\n",
    "                yield(next(evaluate_updates))\n",
    "    \n",
    "    def test_updates(self):\n",
    "        return self.updates(mode=\"test\", update_every=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(BaseTrainer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(Trainer, self).__init__(*args, **kwargs)\n",
    "        \n",
    "    def compute_loss(self, word_embeddings, original_word_embeddings, discriminate_probs): \n",
    "        embeddings_loss = F.cosine_embedding_loss(\n",
    "          word_embeddings.reshape((-1, word_embeddings.shape[2])),\n",
    "          original_word_embeddings.reshape((-1, original_word_embeddings.shape[2])),\n",
    "          torch.ones(word_embeddings.shape[0] * word_embeddings.shape[1]).to(self.device)\n",
    "        )\n",
    "        \n",
    "        discriminator_loss = F.binary_cross_entropy(\n",
    "            discriminate_probs,\n",
    "            torch.zeros_like(discriminate_probs).to(self.device)\n",
    "        )\n",
    "        \n",
    "        return embeddings_loss + discriminator_loss\n",
    "        \n",
    "\n",
    "    def work_batch(self, batch):\n",
    "        word_embeddings, discriminate_probs = self.model(\n",
    "            batch.noisy_word_embeddings,\n",
    "            batch.mode\n",
    "        )\n",
    "\n",
    "        # we're diverging from the article here by outputting the word embeddings\n",
    "        # instead of the probabilities for each word in a vocabulary\n",
    "        # our loss function is using the cosine embedding loss coupled with\n",
    "        # the discriminator loss:\n",
    "        return (\n",
    "            self.compute_loss(word_embeddings, batch.word_embeddings, discriminate_probs),\n",
    "            word_embeddings\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InNotebookTrainer(Trainer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(InNotebookTrainer, self).__init__(*args, **kwargs)\n",
    "        \n",
    "        self.writer = SummaryWriter(comment=self.name)\n",
    "\n",
    "    def train(self, evaluate_every=1000):\n",
    "        test_updates = self.test_updates()\n",
    "        \n",
    "        cumulative_train_metrics = Metrics.empty(mode=\"train\")\n",
    "        cumulative_evaluate_metrics = Metrics.empty(mode=\"eval\")\n",
    "\n",
    "        for update_info in self.train_and_evaluate_updates(evaluate_every=evaluate_every):\n",
    "            if update_info.from_train:\n",
    "                cumulative_train_metrics += update_info.metrics\n",
    "                \n",
    "                print(f\"{update_info.batch.ix}\")\n",
    "                \n",
    "                self.writer.add_scalar(\n",
    "                    'loss/train',\n",
    "                    update_info.metrics.loss,\n",
    "                    update_info.batch.ix\n",
    "                )\n",
    "\n",
    "            if update_info.from_evaluate:\n",
    "                cumulative_evaluate_metrics += update_info.metrics\n",
    "                \n",
    "                self.writer.add_scalar(\n",
    "                    'loss/eval',\n",
    "                    update_info.metrics.loss,\n",
    "                    update_info.batch.ix\n",
    "                )\n",
    "\n",
    "                print(f\"Eval: {update_info.metrics.loss}\")\n",
    "                print(f\"Saving checkpoint\")\n",
    "                self.save_checkpoint()\n",
    "\n",
    "#             if update_info.batch.ix % 1000 == 0 and update_info.batch.ix != 0:\n",
    "#                 test_update = next(test_updates)\n",
    "                \n",
    "#                 self.test_texts_stream.write(\n",
    "#                     (\n",
    "#                         update_info.batch.text,\n",
    "#                         update_info.decoded_inferred_texts\n",
    "#                     )\n",
    "#                 )\n",
    "                \n",
    "    def test(self):\n",
    "        cumulative_metrics = Metrics.empty(mode=\"test\")\n",
    "        \n",
    "        for update_info in self.test_updates():\n",
    "            cumulative_metrics += update_info.metrics\n",
    "\n",
    "        print(cumulative_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_TESTS = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "from hypothesis import given, settings, note, assume, reproduce_failure\n",
    "import hypothesis.strategies as st\n",
    "import hypothesis.extra.numpy as npst\n",
    "\n",
    "class TestNotebook(unittest.TestCase):\n",
    "    def test_trainer_batches_yields_proper_ixs(self):\n",
    "        for mode in ['train', 'test', 'val']:\n",
    "            trainer = Trainer(\n",
    "                'unit-test-run-1',\n",
    "                nlp,\n",
    "                articles,\n",
    "                optimizer_class_name='Adam',\n",
    "                model_args={\n",
    "                    'hidden_size': 128,\n",
    "                    'input_size': 300,\n",
    "                    'num_layers': 2\n",
    "                },\n",
    "                optimizer_args={},\n",
    "                batch_size=32,\n",
    "                update_every=1,\n",
    "                probability_of_mask_for_word=0.3,\n",
    "                device=torch.device('cpu')\n",
    "            )\n",
    "            self.assertGreater(len(trainer.datasets[mode]), 0)\n",
    "            ixs = [batch.ix for batch in itertools.islice(trainer.batches(mode), 10)]\n",
    "            self.assertEqual(list(ixs), list(range(1, 11)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 30.661s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__' and RUN_TESTS:\n",
    "    import doctest\n",
    "    \n",
    "    doctest.testmod()\n",
    "    unittest.main(\n",
    "        argv=['first-arg-is-ignored'],\n",
    "        failfast=True,\n",
    "        exit=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['train', 'val', 'test'], dtype=object)"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# articles.set.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Select a fresh, ripe fruit. Choose a fruit that’s slightly (and evenly) soft, but not mushy. Avoid juicing a fruit that feels hard.The harder the fruit, the more difficult it will be to juice.\\nThe heavier the fruit feels, the juicier it will be!The best citrus fruit has a rind free of blemishes, and a bold, sweet scent.;\\n, Leave it sitting at room temperature until it is no longer cold to the touch.Warm the fruit in the microwave, on high, for twenty to thirty seconds, if desired. Allow it to cool for one minute.Warm fruit is easier to juice.\\nWarming the fruit is especially useful if it was refrigerated.\\n\\n, Hold the fruit against a countertop or other surface. Apply gentle pressure. Roll the fruit back and forth.Rolling the fruit loosens its inner segments, to prepare the fruit for juicing.\\n\\n, Make the cut a bit off-center.Cut the fruit lengthwise if it will fit that way into your squeezer.If not, cut the fruit crosswise, then slice off pointy tip of the rind (if applicable).You can get more juice out of fruits like lemons and limes by cutting them lengthwise instead of crosswise.\\nCutting off the rind tip will give you more leverage for using a manual squeezer.\\n\\n, Make sure to place the cut side of the fruit facing down. The interior dome of the squeezer should press towards the rind-end of the fruit.Placing the cut side face-down may appear counter-intuitive at first. However, if you leave the cut side facing up, the juice will squirt upwards!\\n\\n, Hold the squeezer over a bowl. Place the bottom handle in your non-dominant hand. Press down the top handle with your dominant hand, until the handles are close enough together that you can grip them with both hands at once.If you know exactly how much fruit you need to juice, you can skip the separate container and squeeze directly into a pot or other cookware., Place one hand in front of the other. Grip the handles of the squeezer with both hands at the same time. Squeeze several times, until juice stops flowing from the squeezer.The squeezer will invert the rind so that it’s almost inside-out.\\n\\n  It is normally near the \"delete\", \"end\", and inert key.\\n, It should be above the \"Page Down\" or the PgDn\" key. Pressing this key will result in your camera view going up.'"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(articles.sample(n=2)[\"text\"].fillna('').values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E088] Text of length 198298788 exceeds maximum of 1000000. The v2.x parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-328-d647999f641f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mVocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0marticles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marticles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"headline\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-327-abd231b76108>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, nlp, series)\u001b[0m\n\u001b[1;32m     14\u001b[0m                 \u001b[0mtext\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\" \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mserie\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0mcounts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount_by\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLOWER\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcounts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mix\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcounts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib64/python3.6/site-packages/spacy/language.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m    427\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m             raise ValueError(\n\u001b[0;32m--> 429\u001b[0;31m                 \u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE088\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m             )\n\u001b[1;32m    431\u001b[0m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: [E088] Text of length 198298788 exceeds maximum of 1000000. The v2.x parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`."
     ]
    }
   ],
   "source": [
    "Vocabulary(nlp, [ articles[\"text\"], articles[\"headline\"] ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.46 s, sys: 138 ms, total: 6.6 s\n",
      "Wall time: 6.62 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['.',\n",
       " ',',\n",
       " 'the',\n",
       " 'to',\n",
       " 'you',\n",
       " 'a',\n",
       " 'and',\n",
       " 'your',\n",
       " 'of',\n",
       " 'in',\n",
       " 'it',\n",
       " 'or',\n",
       " '\\n\\n',\n",
       " 'for',\n",
       " '\\n',\n",
       " 'if',\n",
       " 'can',\n",
       " 'is',\n",
       " 'on',\n",
       " 'be',\n",
       " 'that',\n",
       " 'with',\n",
       " 'will',\n",
       " 'are',\n",
       " 'as',\n",
       " 'this',\n",
       " 'have',\n",
       " '-',\n",
       " 'do',\n",
       " 'make',\n",
       " '\\n\\n\\n',\n",
       " 'use',\n",
       " '\"',\n",
       " 'not',\n",
       " 'them',\n",
       " 'they',\n",
       " 'at',\n",
       " '(',\n",
       " ')',\n",
       " 'from',\n",
       " 'out',\n",
       " 'may',\n",
       " 'when',\n",
       " 'should',\n",
       " 'want',\n",
       " 'an',\n",
       " 'like',\n",
       " 'more',\n",
       " 'up',\n",
       " 'also',\n",
       " 'one',\n",
       " 'by',\n",
       " 'about',\n",
       " 'so',\n",
       " \"'s\",\n",
       " \"n't\",\n",
       " 'into',\n",
       " 'sure',\n",
       " '\\n\\n  ',\n",
       " 'other',\n",
       " 'try',\n",
       " 'time',\n",
       " ';',\n",
       " 'some',\n",
       " 'all',\n",
       " 'any',\n",
       " 'but',\n",
       " 'get',\n",
       " 'good',\n",
       " 'help',\n",
       " '“',\n",
       " ':',\n",
       " '”',\n",
       " 'add',\n",
       " 'just',\n",
       " 'need',\n",
       " 'then',\n",
       " '’s',\n",
       " 'n’t',\n",
       " 'way',\n",
       " 'there',\n",
       " 'than',\n",
       " 'water',\n",
       " 'over',\n",
       " 'back',\n",
       " 'their',\n",
       " 'take',\n",
       " 'keep',\n",
       " 'these',\n",
       " 'before',\n",
       " 'around',\n",
       " 'using',\n",
       " 'what',\n",
       " 'look',\n",
       " 'off',\n",
       " 'too',\n",
       " 'many',\n",
       " 'has',\n",
       " 'through',\n",
       " 'go',\n",
       " 'until',\n",
       " 'place',\n",
       " 'find',\n",
       " 'even',\n",
       " 'down',\n",
       " 'top',\n",
       " 'two',\n",
       " 'might',\n",
       " 'see',\n",
       " 'small',\n",
       " 'once',\n",
       " 'hair',\n",
       " 'most',\n",
       " 'work',\n",
       " 'well',\n",
       " 'how',\n",
       " 'side',\n",
       " 'while',\n",
       " 'example',\n",
       " 'which',\n",
       " ' ',\n",
       " 'such',\n",
       " 'few',\n",
       " 'put',\n",
       " 'does',\n",
       " 'each',\n",
       " \"'re\",\n",
       " 'could',\n",
       " 'after',\n",
       " 'something',\n",
       " 'i',\n",
       " 'only',\n",
       " 'ask',\n",
       " 'much',\n",
       " '’re',\n",
       " 'very',\n",
       " 'people',\n",
       " 'right',\n",
       " 'give',\n",
       " 'would',\n",
       " 'oil',\n",
       " 'paper',\n",
       " 'long',\n",
       " '?',\n",
       " 'minutes',\n",
       " 'another',\n",
       " 'yourself',\n",
       " 'set',\n",
       " 'let',\n",
       " 'same',\n",
       " 'know',\n",
       " 'name',\n",
       " 'avoid',\n",
       " '/',\n",
       " 'information',\n",
       " '!',\n",
       " 'different',\n",
       " 'no',\n",
       " 'instead',\n",
       " 'where',\n",
       " 'used',\n",
       " 'because',\n",
       " 'someone',\n",
       " 'color',\n",
       " 'think',\n",
       " 'part',\n",
       " 'area',\n",
       " 'remove',\n",
       " 'head',\n",
       " 'items',\n",
       " 'being',\n",
       " 'little',\n",
       " \"'ll\",\n",
       " 'usually',\n",
       " 'heat',\n",
       " 'both',\n",
       " '1',\n",
       " 'say',\n",
       " 'however',\n",
       " 'click',\n",
       " 'always',\n",
       " 'less',\n",
       " 'show',\n",
       " 'start',\n",
       " 'end',\n",
       " 'experience',\n",
       " 'possible',\n",
       " 'he',\n",
       " 'great',\n",
       " 'things',\n",
       " 'feel',\n",
       " 'lot',\n",
       " 'cover',\n",
       " 'move',\n",
       " 'its',\n",
       " 'child',\n",
       " 'his',\n",
       " 'turn',\n",
       " 'away',\n",
       " '}',\n",
       " 'include',\n",
       " 'new',\n",
       " 'clean',\n",
       " 'parents',\n",
       " 'first',\n",
       " 'cut',\n",
       " 'made',\n",
       " 'paint',\n",
       " 'day',\n",
       " 'method',\n",
       " 'wear',\n",
       " 'fabric',\n",
       " 'vacuum',\n",
       " 'hand',\n",
       " 'person',\n",
       " 'making',\n",
       " 'able',\n",
       " 'own',\n",
       " 'baking',\n",
       " 'check',\n",
       " 'apply',\n",
       " 'home',\n",
       " 'him',\n",
       " 'her',\n",
       " 'now',\n",
       " 'create',\n",
       " 'glue',\n",
       " 'children',\n",
       " 'remember',\n",
       " 'three',\n",
       " 'easily',\n",
       " 'room',\n",
       " 'high',\n",
       " 'hot',\n",
       " 'number',\n",
       " 'ham',\n",
       " 'choose',\n",
       " 'colors',\n",
       " 'face',\n",
       " 'enough',\n",
       " 'open',\n",
       " 'skin',\n",
       " 'ferret',\n",
       " 'best',\n",
       " 'dry',\n",
       " 'food',\n",
       " 'really',\n",
       " 'cup',\n",
       " 'still',\n",
       " 'begin',\n",
       " 'often',\n",
       " 'amount',\n",
       " 'black',\n",
       " 'clothes',\n",
       " 'hard',\n",
       " 'every',\n",
       " 'large',\n",
       " 'credit',\n",
       " 'dog',\n",
       " 'important',\n",
       " 'house',\n",
       " 'wrap',\n",
       " 'white',\n",
       " 'list',\n",
       " 'potatoes',\n",
       " 'x',\n",
       " '’ll',\n",
       " 'teeth',\n",
       " 'health',\n",
       " 'bit',\n",
       " 'during',\n",
       " 'close',\n",
       " 'body',\n",
       " 'store',\n",
       " 'message',\n",
       " 'tomatoes',\n",
       " 'line',\n",
       " 'pieces',\n",
       " 'ovens',\n",
       " 'talk',\n",
       " 'pour',\n",
       " 'parts',\n",
       " 'cause',\n",
       " 'who',\n",
       " 'allow',\n",
       " 'again',\n",
       " 'leave',\n",
       " 'wo',\n",
       " 'doing',\n",
       " 'better',\n",
       " 'mixture',\n",
       " 'onto',\n",
       " 'piece',\n",
       " 'options',\n",
       " 'plants',\n",
       " 'mind',\n",
       " 'book',\n",
       " 'available',\n",
       " 'outside',\n",
       " 'order',\n",
       " 'lower',\n",
       " 'plastic',\n",
       " 'google',\n",
       " 'page',\n",
       " 'without',\n",
       " 'tomato',\n",
       " 'gently',\n",
       " 'between',\n",
       " 'code',\n",
       " 'pressure',\n",
       " 'left',\n",
       " 'appear',\n",
       " 'change',\n",
       " 'soda',\n",
       " 'mix',\n",
       " 'friend',\n",
       " 'tell',\n",
       " 'call',\n",
       " 'search',\n",
       " 'phone',\n",
       " 'option',\n",
       " 'access',\n",
       " 'been',\n",
       " 'ground',\n",
       " 'front',\n",
       " 'consider',\n",
       " 'under',\n",
       " 'piano',\n",
       " 'focus',\n",
       " 'next',\n",
       " \"'\",\n",
       " 'process',\n",
       " 'draw',\n",
       " 'certain',\n",
       " 'buy',\n",
       " 'together',\n",
       " 'form',\n",
       " 'come',\n",
       " '3',\n",
       " 'blue',\n",
       " 'tap',\n",
       " 'address',\n",
       " 'bottom',\n",
       " 'brush',\n",
       " 'she',\n",
       " 'least',\n",
       " 'potato',\n",
       " 'word',\n",
       " 'words',\n",
       " 'wine',\n",
       " 'section',\n",
       " 'likely',\n",
       " 'warm',\n",
       " 'window',\n",
       " 'bring',\n",
       " 'read',\n",
       " 'longer',\n",
       " 'going',\n",
       " 'stay',\n",
       " 'red',\n",
       " 'life',\n",
       " 'old',\n",
       " 'light',\n",
       " 'point',\n",
       " '2',\n",
       " 'size',\n",
       " 'ensure',\n",
       " 'oven',\n",
       " 'loose',\n",
       " 'comes',\n",
       " 'icon',\n",
       " 'continue',\n",
       " 'contact',\n",
       " 'pizza',\n",
       " 'sauce',\n",
       " 'box',\n",
       " 'simply',\n",
       " 'ribbon',\n",
       " 'rinse',\n",
       " 'especially',\n",
       " 'etc',\n",
       " \"'ve\",\n",
       " 'times',\n",
       " 'picture',\n",
       " 'works',\n",
       " 'sign',\n",
       " 'sit',\n",
       " 'stomach',\n",
       " 'regular',\n",
       " 'dish',\n",
       " 'paste',\n",
       " 'looking',\n",
       " 'free',\n",
       " 'center',\n",
       " 'completely',\n",
       " 'smaller',\n",
       " 'keyboard',\n",
       " 'weight',\n",
       " 'makes',\n",
       " 'position',\n",
       " 'state',\n",
       " 'button',\n",
       " 'service',\n",
       " 'wire',\n",
       " '–',\n",
       " 'getting',\n",
       " 'style',\n",
       " 'valance',\n",
       " 'wrens',\n",
       " 'others',\n",
       " 'pay',\n",
       " 'car',\n",
       " 'else',\n",
       " 'problem',\n",
       " 'simple',\n",
       " 'glass',\n",
       " 'adding',\n",
       " 'half',\n",
       " 'cool',\n",
       " 'idea',\n",
       " 'teacher',\n",
       " 'must',\n",
       " 'value',\n",
       " 'symptoms',\n",
       " 'extra',\n",
       " 'pull',\n",
       " 'ways',\n",
       " 'nose',\n",
       " 'online',\n",
       " 'shape',\n",
       " 'oregano',\n",
       " 'questions',\n",
       " 'hold',\n",
       " 'since',\n",
       " 'lose',\n",
       " 'fold',\n",
       " 'along',\n",
       " 'run',\n",
       " 'hours',\n",
       " 'card',\n",
       " 'opponent',\n",
       " 'ice',\n",
       " 'school',\n",
       " 'slices',\n",
       " 'rod',\n",
       " 'seeds',\n",
       " 'seizure',\n",
       " 'hamster',\n",
       " 'kind',\n",
       " 'rather',\n",
       " 'never',\n",
       " 'treat',\n",
       " 'easy',\n",
       " 'books',\n",
       " 'screen',\n",
       " 'full',\n",
       " 'relationship',\n",
       " 'support',\n",
       " 'buddhist',\n",
       " 'step',\n",
       " 'dye',\n",
       " 'safe',\n",
       " 'temperature',\n",
       " 'stir',\n",
       " 'probably',\n",
       " 'either',\n",
       " 'item',\n",
       " 'enter',\n",
       " 'press',\n",
       " 'anything',\n",
       " 'medium',\n",
       " 'pot',\n",
       " 'board',\n",
       " 'easier',\n",
       " 'vehicle',\n",
       " 'done',\n",
       " 'wash',\n",
       " '’ve',\n",
       " 'hang',\n",
       " 'stores',\n",
       " 'slightly',\n",
       " 'eyes',\n",
       " 'near',\n",
       " 'eyeliner',\n",
       " 'those',\n",
       " 'care',\n",
       " 'tend',\n",
       " 'friends',\n",
       " 'maybe',\n",
       " 'control',\n",
       " 'clear',\n",
       " 'salt',\n",
       " 'already',\n",
       " 'practice',\n",
       " 'actually',\n",
       " 'fresh',\n",
       " 'local',\n",
       " 'g',\n",
       " 'sides',\n",
       " 'depending',\n",
       " 'inside',\n",
       " 'carefully',\n",
       " 'spray',\n",
       " 'hands',\n",
       " 'basic',\n",
       " 'money',\n",
       " 'main',\n",
       " 'select',\n",
       " 'favorite',\n",
       " 'file',\n",
       " 'bowl',\n",
       " 'smooth',\n",
       " '6',\n",
       " 'last',\n",
       " 'prevent',\n",
       " 'sense',\n",
       " 'here',\n",
       " 'reach',\n",
       " 'cd',\n",
       " 'settings',\n",
       " 'natural',\n",
       " 'type',\n",
       " 'stick',\n",
       " 'specific',\n",
       " 'job',\n",
       " 'year',\n",
       " 'areas',\n",
       " 'bigger',\n",
       " 'repeat',\n",
       " 'copper',\n",
       " 'algorithm',\n",
       " 'straight',\n",
       " 'feet',\n",
       " 'chopped',\n",
       " 'stain',\n",
       " 'ca',\n",
       " 'offer',\n",
       " 'me',\n",
       " 'taking',\n",
       " 'juice',\n",
       " 'fill',\n",
       " 'flat',\n",
       " 'normal',\n",
       " 'variety',\n",
       " 'having',\n",
       " 'difficult',\n",
       " 'big',\n",
       " 'though',\n",
       " 'chickens',\n",
       " 'surface',\n",
       " 'stop',\n",
       " 'mouth',\n",
       " 'bite',\n",
       " 'apple',\n",
       " 'directly',\n",
       " 'middle',\n",
       " 'cost',\n",
       " 'comfortable',\n",
       " 'working',\n",
       " 'crust',\n",
       " 'interested',\n",
       " '4',\n",
       " 'square',\n",
       " 'serve',\n",
       " 'document',\n",
       " 'ring',\n",
       " 'muscles',\n",
       " 'motion',\n",
       " 'switch',\n",
       " 'following',\n",
       " 'short',\n",
       " 'detergent',\n",
       " 'space',\n",
       " 'clip',\n",
       " 'needed',\n",
       " 'towel',\n",
       " 'combine',\n",
       " 'sound',\n",
       " 'length',\n",
       " '10',\n",
       " 'insert',\n",
       " 'character',\n",
       " 'copy',\n",
       " 'magnets',\n",
       " 'pie',\n",
       " 'cleaner',\n",
       " 'nail',\n",
       " 'nests',\n",
       " 'torch',\n",
       " 'dwarf',\n",
       " 'link',\n",
       " 'asking',\n",
       " 'menu',\n",
       " 'calm',\n",
       " 'lay',\n",
       " 'period',\n",
       " 'notice',\n",
       " 'filling',\n",
       " 'milk',\n",
       " 'play',\n",
       " 'skype',\n",
       " 'allows',\n",
       " 'cheese',\n",
       " '8',\n",
       " 'anyone',\n",
       " 'later',\n",
       " 'push',\n",
       " 'cold',\n",
       " 'soft',\n",
       " 'cutting',\n",
       " 'second',\n",
       " 'names',\n",
       " 'correct',\n",
       " 'improve',\n",
       " 'plan',\n",
       " 'legal',\n",
       " 'treatment',\n",
       " 'damage',\n",
       " 'pinworms',\n",
       " 'mice',\n",
       " 'single',\n",
       " 'pin',\n",
       " 'hole',\n",
       " 'although',\n",
       " 'including',\n",
       " 'leather',\n",
       " 'activity',\n",
       " 'inches',\n",
       " 'ride',\n",
       " 'mount',\n",
       " 'cleaning',\n",
       " 'security',\n",
       " 'based',\n",
       " 'pick',\n",
       " 'breathing',\n",
       " 'gerbil',\n",
       " 'partner',\n",
       " 'shaving',\n",
       " 'foods',\n",
       " 'listen',\n",
       " 'creating',\n",
       " 'above',\n",
       " 'appreciate',\n",
       " 'everyone',\n",
       " 'thank',\n",
       " 'added',\n",
       " 'teen',\n",
       " 'several',\n",
       " 'website',\n",
       " 'lots',\n",
       " 'love',\n",
       " 'lead',\n",
       " 'years',\n",
       " 'means',\n",
       " 'liquid',\n",
       " 'far',\n",
       " 'minute',\n",
       " 'neck',\n",
       " '5',\n",
       " 'bad',\n",
       " 'taste',\n",
       " 'password',\n",
       " 'cook',\n",
       " 'diet',\n",
       " '\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n',\n",
       " 'toppings',\n",
       " 'stand',\n",
       " 'cm',\n",
       " 'row',\n",
       " 'quality',\n",
       " 'entire',\n",
       " 'key',\n",
       " 'services',\n",
       " 'additional',\n",
       " 'alcohol',\n",
       " 'setting',\n",
       " 'keeping',\n",
       " 'require',\n",
       " 'braid',\n",
       " 'fun',\n",
       " 'edge',\n",
       " 'break',\n",
       " 'serious',\n",
       " 'slide',\n",
       " 'bankruptcy',\n",
       " 'dip',\n",
       " 'slowly',\n",
       " 'effective',\n",
       " 'ingredients',\n",
       " 'sink',\n",
       " 'return',\n",
       " 'expensive',\n",
       " 'quickly',\n",
       " 'moving',\n",
       " 'gain',\n",
       " 'companies',\n",
       " 'was',\n",
       " 'write',\n",
       " 'wet',\n",
       " 'cream',\n",
       " 'purchase',\n",
       " 'shot',\n",
       " 'against',\n",
       " 'legs',\n",
       " 'mark',\n",
       " 'real',\n",
       " 'music',\n",
       " 'evenly',\n",
       " 'tools',\n",
       " 'wires',\n",
       " 'power',\n",
       " 'negative',\n",
       " 'syrup',\n",
       " 'solution',\n",
       " 'party',\n",
       " 'brackets',\n",
       " 'devices',\n",
       " 'dummy',\n",
       " 'certificate',\n",
       " '--',\n",
       " 'goes',\n",
       " 'attention',\n",
       " 'nice',\n",
       " 'behind',\n",
       " 'thing',\n",
       " 'wait',\n",
       " 'foot',\n",
       " 'mouse',\n",
       " 'ml',\n",
       " 'sex',\n",
       " 'direct',\n",
       " 'family',\n",
       " 'within',\n",
       " 'tan',\n",
       " 'walk',\n",
       " 'bright',\n",
       " 'travel',\n",
       " 'meditation',\n",
       " 'five',\n",
       " 'sell',\n",
       " 'eggs',\n",
       " 'frame',\n",
       " 'pregnancy',\n",
       " 'fish',\n",
       " '30',\n",
       " 'level',\n",
       " 'needs',\n",
       " 'trying',\n",
       " 'likes',\n",
       " 'yet',\n",
       " 'connect',\n",
       " 'eat',\n",
       " 'chicken',\n",
       " 'cooking',\n",
       " 'round',\n",
       " 'spoon',\n",
       " 'layer',\n",
       " 'thick',\n",
       " 'drive',\n",
       " 'benefit',\n",
       " 'follow',\n",
       " 'build',\n",
       " 'hear',\n",
       " 'attach',\n",
       " 'case',\n",
       " 'original',\n",
       " 'letter',\n",
       " 'learn',\n",
       " 'note',\n",
       " 'potential',\n",
       " 'common',\n",
       " 'lost',\n",
       " 'my',\n",
       " 'test',\n",
       " 'bottle',\n",
       " 'vertical',\n",
       " 'facing',\n",
       " 'court',\n",
       " 'self',\n",
       " 'feature',\n",
       " 'debt',\n",
       " 'drops',\n",
       " 'lemon',\n",
       " 'vinegar',\n",
       " 'degree',\n",
       " 'clothing',\n",
       " 'multiple',\n",
       " 'double',\n",
       " 'noise',\n",
       " 'sharp',\n",
       " 'metal',\n",
       " 'range',\n",
       " 'everything',\n",
       " 'spot',\n",
       " 'addition',\n",
       " 'career',\n",
       " 'why',\n",
       " 'dirty',\n",
       " 'dirt',\n",
       " 'air',\n",
       " 'enjoy',\n",
       " 'designs',\n",
       " 'traditional',\n",
       " 'barrel',\n",
       " 'marigolds',\n",
       " 'days',\n",
       " 'protect',\n",
       " 'got',\n",
       " 'characters',\n",
       " 'force',\n",
       " 'encryption',\n",
       " 'parent',\n",
       " 'justin',\n",
       " 'sugar',\n",
       " 'leg',\n",
       " 'paprika',\n",
       " 'sweet',\n",
       " 'charcoal',\n",
       " 'products',\n",
       " 'wearing',\n",
       " 'shorts',\n",
       " 'maple',\n",
       " 'nuts',\n",
       " 'battery',\n",
       " 'death',\n",
       " 'shipping',\n",
       " 'hipaa',\n",
       " 'healthcare',\n",
       " 'bikini',\n",
       " 'browser',\n",
       " 'talking',\n",
       " 'towards',\n",
       " 'offers',\n",
       " 'afraid',\n",
       " 'mean',\n",
       " 'alone',\n",
       " 'adult',\n",
       " 'tanning',\n",
       " 'careful',\n",
       " 'burn',\n",
       " 'baby',\n",
       " 'strong',\n",
       " 'found',\n",
       " 'spend',\n",
       " 'material',\n",
       " 'purpose',\n",
       " 'wipe',\n",
       " 'brown',\n",
       " 'takes',\n",
       " 'mild',\n",
       " 'shiny',\n",
       " 'whole',\n",
       " 'rest',\n",
       " 'finished',\n",
       " 'grab',\n",
       " 'hit',\n",
       " 'device',\n",
       " 'corner',\n",
       " 'email',\n",
       " 'account',\n",
       " 'protein',\n",
       " 'spread',\n",
       " '\\n\\n\\n\\n\\n\\n\\n\\n\\n',\n",
       " 'personal',\n",
       " 'low',\n",
       " 'drain',\n",
       " '20',\n",
       " 'files',\n",
       " 'date',\n",
       " 'understand',\n",
       " 'necessary',\n",
       " 'aside',\n",
       " '1/2',\n",
       " 'flour',\n",
       " 'starting',\n",
       " 'hide',\n",
       " 'provide',\n",
       " 'driver',\n",
       " 'exactly',\n",
       " 'fix',\n",
       " 'helps',\n",
       " 'flavor',\n",
       " 'tight',\n",
       " 'determine',\n",
       " 'strip',\n",
       " 'version',\n",
       " 'direction',\n",
       " 'bill',\n",
       " 'sized',\n",
       " 'covers',\n",
       " 'running',\n",
       " 'live',\n",
       " 'wind',\n",
       " 'regularly',\n",
       " 'looks',\n",
       " 'folder',\n",
       " 'clicking',\n",
       " 'container',\n",
       " 'problems',\n",
       " 'drop',\n",
       " 'pair',\n",
       " 'neutral',\n",
       " 'result',\n",
       " 'places',\n",
       " '$',\n",
       " 'company',\n",
       " 'cloth',\n",
       " 'creative',\n",
       " 'chocolate',\n",
       " 'save',\n",
       " 'building',\n",
       " 'seat',\n",
       " 'increase',\n",
       " 'design',\n",
       " 'growing',\n",
       " 'soil',\n",
       " 'companion',\n",
       " 'office',\n",
       " 'general',\n",
       " 'data',\n",
       " 'night',\n",
       " 'wall',\n",
       " 'gentle',\n",
       " 'project',\n",
       " 'predators',\n",
       " 'reduce',\n",
       " 'lines',\n",
       " 'overripe',\n",
       " 'dress',\n",
       " 'candy',\n",
       " 'dolly',\n",
       " 'mail',\n",
       " 'covered',\n",
       " 'fruit',\n",
       " 'computer',\n",
       " 'dentist',\n",
       " 'overbite',\n",
       " 'smoothies',\n",
       " 'stamp',\n",
       " 'tampon',\n",
       " 'minecraft',\n",
       " 'saying',\n",
       " 'generally',\n",
       " 'did',\n",
       " 'seconds',\n",
       " 'particular',\n",
       " 'advice',\n",
       " 'figure',\n",
       " 'judge',\n",
       " 'drawing',\n",
       " 'effects',\n",
       " 'sun',\n",
       " 'hour',\n",
       " 'fit',\n",
       " 'grow',\n",
       " 'vegetables',\n",
       " 'sometimes',\n",
       " 'physical',\n",
       " 'blood',\n",
       " 'solid',\n",
       " 'gas',\n",
       " 'knife',\n",
       " 'eye',\n",
       " 'drag',\n",
       " 'across',\n",
       " 'floor',\n",
       " 'toes',\n",
       " 'tip',\n",
       " 'installed',\n",
       " 'download',\n",
       " 'video',\n",
       " '\\n\\n\\n\\n',\n",
       " 'tab',\n",
       " 'forget',\n",
       " 'whatever',\n",
       " 'healthy',\n",
       " 'thin',\n",
       " 'ahead',\n",
       " 'pre',\n",
       " 'dough',\n",
       " 'guide',\n",
       " 'were',\n",
       " 'recipe',\n",
       " '1/4',\n",
       " 'c',\n",
       " '24',\n",
       " 'separate',\n",
       " '100',\n",
       " 'particularly',\n",
       " 'focused',\n",
       " 'movements',\n",
       " 'lights',\n",
       " 'costs',\n",
       " 'price',\n",
       " 'larger',\n",
       " 'action',\n",
       " 'past',\n",
       " 'green',\n",
       " 'serotonin',\n",
       " 'syndrome',\n",
       " ...]"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "text = ' '.join(articles.sample(n=200)[\"text\"].fillna('').values.tolist())\n",
    "counts = nlp(text).count_by(spacy.attrs.LOWER)\n",
    "\n",
    "[\n",
    "    nlp.vocab[ix].text\n",
    "    for ix, _ in sorted(\n",
    "        [(ix, counts[ix]) for ix in counts],\n",
    "        key=lambda t: t[1],\n",
    "        reverse=True\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this'"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab[1995909169258310477].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "# trainer = InNotebookTrainer(\n",
    "#     'test-run-1',\n",
    "#     nlp,\n",
    "#     articles,\n",
    "#     optimizer_class_name='Adam',\n",
    "#     model_args={\n",
    "#         'hidden_size': 128,\n",
    "#         'input_size': 300,\n",
    "#         'num_layers': 2\n",
    "#     },\n",
    "#     optimizer_args={},\n",
    "#     batch_size=1,\n",
    "#     update_every=1,\n",
    "#     probability_of_mask_for_word=0.2,\n",
    "#     device=torch.device('cuda')\n",
    "# )\n",
    "\n",
    "# update_info = next(trainer.updates('train'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['jean-clad46364636463646364636464246424642464212dpo12dpo4642464246424642464246424642464246424642464246424642464246424642464246424642464246424642464246424642464046404640464046404642hpthpt464246424642464246424642464246424642464246424642464246424642464212dpo12dpostigmatismstigmatismstigmatismstigmatismstigmatism464246424642464246424642464246364636463646364642463646364636463646364642464246425.3335.3334642464046404640464046404640464046404640464046424642464246424642464246424642464246424642464246424642464246424642464246364636463646364642464046404640464046404640464046404640464246424642464246424642464246424642464246424640464046424642464246404640464012dpo12dpo4642464246424642464246424640464046405.3335.3335.3334642464046404640464246424640464246424642jean-cladjean-clad46424642464246424642464246424642464246424642464246424642stigmatismstigmatismstigmatism46424640464046404640464046404640464046404642464246424642464246424642464246424642464246424642stigmatism12dpo46404640464046404640464246424642464246425.3335.3335.3335.333quesidillastigmatism12dpo12dpo46404640464046404640464046404640464246424642464246424642464246424642464246424640464046404640464046404640464046404640464046424642464246404640464046404642464246424642464246424642464246424642464246424642464246424642464246424642464246424642464246424642decolonizeddecolonizeddecolonizeddecolonizeddecolonized4640464046404640464046404640464046404640464246424642464246424642464246424642464246424642464246424642464246424642464212dpo12dpo464046404640464046404640464046404640464046364636464246424642464246424642464046404640464046404642464246424642464246424642464246404640464046424642464246424640464046404640464046404640464246424642464246424642464246424642464246424642464246424642stigmatismstigmatismstigmatismjean-cladjean-claddecolonizeddecolonizeddecolonizeddecolonized46424642464246404640464046404640464246364636463612dpo12dpo4642hpt464246404640464046404640464046404640464046404640464046404642464246424642464046404640464046404636464246424642464246404640464046404640464246424640464046404640464046404640decolonizeddecolonizeddecolonizeddecolonizeddecolonizeddecolonized4642464246424642464246424640464046404642464046404640464046424642464246424642464046404640464046404640464046404640464046424642464246424642464246424642decolonizeddecolonizeddecolonizeddecolonizeddecolonized4642464246424642464212dpo12dpo12dpo12dpo464246424642464246424642464246424642464246424642464246424642464246424642464246424642464246424642464246424642464246424642464246424642decolonizeddecolonized4642decolonized4642464246424642464246424642464246424642464246424642464246424642464246424642464246424642464246424640464046404640464046424642464246404640464046424642464246424642464246424642464246424642463646364640464046404640positiepositiepositie12dpo12dpo46404640decolonizeddecolonizeddecolonizeddecolonized46424642libfoo']\n",
      "CPU times: user 16.1 s, sys: 20.2 s, total: 36.3 s\n",
      "Wall time: 21.1 s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "\n",
    "# print(update_info.decoded_inferred_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not RUN_TESTS:\n",
    "    if 'trainer' in vars():\n",
    "        print(f\"About to delete old trainer\")\n",
    "        del trainer\n",
    "\n",
    "    trainer = InNotebookTrainer(\n",
    "        'test-run-1',\n",
    "        nlp,\n",
    "        articles,\n",
    "        optimizer_class_name='Adam',\n",
    "        model_args={\n",
    "            'hidden_size': 128,\n",
    "            'input_size': 300,\n",
    "            'num_layers': 2\n",
    "        },\n",
    "        optimizer_args={},\n",
    "        batch_size=32,\n",
    "        update_every=1,\n",
    "        probability_of_mask_for_word=0.2,\n",
    "        device=torch.device('cuda')\n",
    "    )\n",
    "\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-270-1c4df96493b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_batch_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'trainer' is not defined"
     ]
    }
   ],
   "source": [
    "trainer.current_batch_id"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
