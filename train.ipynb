{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import functools\n",
    "import itertools\n",
    "import random\n",
    "import math\n",
    "import pdb\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import spacy\n",
    "from cached_property import cached_property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\n",
    "    \"en_core_web_lg\",\n",
    "    disable=[\"tagger\", \"ner\", \"textcat\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = pd.read_parquet(\"data/articles-processed.parquet.gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscriminatorNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DiscriminatorNet, self).__init__()\n",
    "\n",
    "    def forward(self, hidden_state):\n",
    "        \"\"\"\n",
    "        The forward pass for the network\n",
    "        \n",
    "        hidden_state : tensor (batch_num, hidden_size)\n",
    "        \n",
    "        returns         : tensor (batch_num, 1)\n",
    "        \"\"\"\n",
    "        \n",
    "        return hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SummarizeNet(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(SummarizeNet, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "    def forward(self, word_embeddings, generate=\"sentence\"):\n",
    "        \"\"\"\n",
    "        The forward pass for the network\n",
    "        \n",
    "        word_embeddings : tensor (batch_num, max_seq_len, vocab_len)\n",
    "        \n",
    "        returns         : tuple (\n",
    "                            tensor (batch_num, max_seq_len, vocab_len),\n",
    "                            tensor (batch_num, hidden_size)\n",
    "                          )\n",
    "        \n",
    "        First tensor in the returning tuple is a probability over the vocabulary\n",
    "        for each sequence position\n",
    "        \n",
    "        The second tensor is an encoder's hidden state \n",
    "        \"\"\"\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArticlesDataset(Dataset):\n",
    "    def __init__(self, dataframe, mode, transforms=[]):\n",
    "        self.data = dataframe[dataframe.set == mode]\n",
    "        self.transforms = transforms\n",
    "        self.mode = mode\n",
    "        \n",
    "    def __len__(self):\n",
    "        return 2*len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        _idx = []\n",
    "        \n",
    "        if torch.is_tensor(idx):\n",
    "            _idx = idx.tolist()\n",
    "        \n",
    "        if isinstance(idx, list):\n",
    "            _idx = idx\n",
    "        else:\n",
    "            _idx = [ idx ]\n",
    "        \n",
    "        _ids = [ (i - (i % 2))/2 for i in _idx]\n",
    "\n",
    "        data = self.data.iloc[_ids, :]\n",
    "        data['asked_id'] = _idx\n",
    "        \n",
    "        data = pd.DataFrame(\n",
    "            {\n",
    "                'set': [self.mode for _ in range(0, len(_ids))],\n",
    "                'mode': [ (0 if i % 2 == 0 else 1) for i in _idx ],\n",
    "                'text': data.apply(lambda row: row['text'] if row['asked_id'] % 2 == 0 else row['headline'], axis=1),\n",
    "                'title': data['normalized_title']\n",
    "            }\n",
    "        )\n",
    "\n",
    "        for transform in self.transforms:\n",
    "            data = transform(data)\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextToParsedDoc(object):\n",
    "    def __init__(self, nlp):\n",
    "        self.nlp = nlp\n",
    "        \n",
    "    def __call__(self, sample):\n",
    "        sample['doc'] = sample.apply(lambda row: self.nlp(row['text']), axis=1)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordsToVectors(object):\n",
    "    def __init__(self, nlp):\n",
    "        self.nlp = nlp\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        \n",
    "        sample['word_embeddings'] = sample.apply(\n",
    "            lambda row: np.stack([token.vector for token in row['doc']]),\n",
    "            axis=1\n",
    "        )\n",
    "    \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddNoiseToEmbeddings(object):\n",
    "    def __init__(self, probability_of_mask_for_word):\n",
    "        self.probability_of_mask_for_word = probability_of_mask_for_word\n",
    "        self.rng = np.random.default_rng()\n",
    "        \n",
    "    def mask_vector(self, vector):\n",
    "        \"\"\"\n",
    "        Masks words with zeros randomly\n",
    "        \"\"\"\n",
    "        seq_len = vector.shape[0]\n",
    "        vector_len = vector.shape[1]\n",
    "        \n",
    "        mask = np.repeat(\n",
    "            self.rng.choice(\n",
    "                [0, 1],\n",
    "                seq_len,\n",
    "                p=[\n",
    "                    self.probability_of_mask_for_word,\n",
    "                    (1 - self.probability_of_mask_for_word)\n",
    "                ]\n",
    "            ).reshape((seq_len, 1)),\n",
    "            vector_len,\n",
    "            axis=1\n",
    "        )\n",
    "        \n",
    "        return vector * mask\n",
    "        \n",
    "    def __call__(self, sample):       \n",
    "        sample['noisy_word_embeddings'] = sample['word_embeddings'].apply(self.mask_vector)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MergeBatch(object):\n",
    "    def stack_vectors(self, vectors):\n",
    "        max_seq = max([vector.shape[0] for vector in vectors])\n",
    "        \n",
    "        return np.stack(\n",
    "            [\n",
    "                np.pad(vector, [(0, max_seq - vector.shape[0]), (0, 0)])\n",
    "                for vector in vectors\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "    def __call__(self, sample):\n",
    "        del sample['doc']\n",
    "        \n",
    "        sample = sample.to_dict(orient=\"list\")\n",
    "        \n",
    "        sample['word_embeddings'] = self.stack_vectors(sample['word_embeddings'])\n",
    "        sample['noisy_word_embeddings'] = self.stack_vectors(sample['noisy_word_embeddings'])\n",
    "    \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader(object):\n",
    "    def __init__(self, dataset, batch_size=8, num_workers=1):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        \n",
    "    @property\n",
    "    def epoch_size(self):\n",
    "        return math.ceil(len(self.dataset) / self.batch_size) * self.batch_size\n",
    "    \n",
    "    def __iter__(self):\n",
    "        ids = random.choices(range(0, len(self.dataset)), k=self.epoch_size)\n",
    "        \n",
    "        for start_ix in range(0, self.epoch_size, self.batch_size):\n",
    "            yield self.dataset[ids[start_ix:(start_ix + self.batch_size)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArticlesBatch:\n",
    "    def __init__(self, data, id):\n",
    "        self.data = data\n",
    "        self.id = id\n",
    "        \n",
    "    def __getattr__(self, name):\n",
    "        if name in self.data:\n",
    "            return self.data[name]\n",
    "        else:\n",
    "            raise AttributeError(f\"Attribute missing: {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpdateInfo(object):\n",
    "    def __init__(self, batch, loss_sum, mode):\n",
    "        self.batch = batch\n",
    "        self.loss_sum = loss_sum\n",
    "        self.mode = mode\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"{self.mode} | {self.batch.id}\\t| Loss: {loss_sum}\\t\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseTrainer:\n",
    "    def __init__(self, name, nlp, dataframe):\n",
    "        self.name = name\n",
    "        \n",
    "        self.datasets = {\n",
    "            \"train\": ArticlesDataset(\n",
    "                dataframe,\n",
    "                \"train\",\n",
    "                transforms=[\n",
    "                    TextToParsedDoc(nlp),\n",
    "                    WordsToVectors(nlp),\n",
    "                    AddNoiseToEmbeddings(probability_of_mask_for_word),\n",
    "                    MergeBatch()\n",
    "                ]\n",
    "            ),\n",
    "            \"test\":  ArticlesDataset(\n",
    "                dataframe,\n",
    "                \"test\",\n",
    "                transforms=[\n",
    "                    TextToParsedDoc(nlp),\n",
    "                    WordsToVectors(nlp),\n",
    "                    AddNoiseToEmbeddings(0),\n",
    "                    MergeBatch()\n",
    "                ]\n",
    "            ),\n",
    "            \"eval\":  ArticlesDataset(\n",
    "                dataframe,\n",
    "                \"eval\",\n",
    "                transforms=[\n",
    "                    TextToParsedDoc(nlp),\n",
    "                    WordsToVectors(nlp),\n",
    "                    AddNoiseToEmbeddings(0),\n",
    "                    MergeBatch()\n",
    "                ]\n",
    "            )\n",
    "        }\n",
    "        \n",
    "        self.current_batch_id = 0\n",
    "        \n",
    "    def configure(self, summarize_model_kwargs, discriminate_model_kwargs,\n",
    "                 batch_size, update_every, save_every, loader_workers,\n",
    "                 probability_of_mask_for_word):\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.update_every = update_every\n",
    "        self.save_every = save_every\n",
    "        self.loader_workers = loader_workers\n",
    "        \n",
    "        self.summarize_model_kwargs = summarize_model_kwargs\n",
    "        self.discriminate_model_kwargs = discriminate_model_kwargs\n",
    "        \n",
    "    @cachedproperty\n",
    "    def summarize_model(self):\n",
    "        pass\n",
    "    \n",
    "    @cachedproperty\n",
    "    def discriminate_model(self):\n",
    "        pass\n",
    "    \n",
    "    def save_checkpoint(self):\n",
    "        checkpoint_path = f\"checkpoints/{self.name}/batch-#{self.current_batch_id}\"\n",
    "        os.makedirs(checkpoint_path, exist_ok=True)\n",
    "        \n",
    "        torch.save(\n",
    "            {\n",
    "                'current_batch_id': self.current_batch_id,\n",
    "                'batch_size': self.batch_size,\n",
    "                'update_every': self.update_every,\n",
    "                'save_every': self.save_every,\n",
    "                'lambda_article': self.lambda_article,\n",
    "                'lambda_sentence': self.lambda_sentence,\n",
    "                'summarize_model_state': self.summarize_model.state_dict(),\n",
    "                'discriminate_model_state': self.discriminate_model.state_dict(),\n",
    "                'optimizer_state_dict': self.optimizer.state_dict()\n",
    "            },\n",
    "            f\"{checkpoint_path}/state.pth\"\n",
    "        )\n",
    "        \n",
    "    def load_checkpoint(name, dataframe):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def batches(self, mode):\n",
    "        start_id = self.current_batch_id\n",
    "        \n",
    "        while True:\n",
    "            loader = DataLoader(\n",
    "                self.datasets[mode],\n",
    "                batch_size=self.batch_size,\n",
    "                num_workers=self.loader_workers\n",
    "            )\n",
    "\n",
    "            for ix, data in enumerate(loader):\n",
    "                self.current_batch_id += ix\n",
    "                \n",
    "                yield(\n",
    "                    ArticlesBatch(\n",
    "                        data,\n",
    "                        id=self.current_batch_id\n",
    "                    )\n",
    "                )\n",
    "        \n",
    "    \n",
    "    def batch_loss(self, batch):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def train_and_evaluate_updates(self, evaluate_every=100):\n",
    "        train_updates = self.updates(mode=\"train\")\n",
    "        evaluate_updates = self.updates(mode=\"eval\")\n",
    "        \n",
    "        for update_info in train_updates:\n",
    "            yield(update_info)\n",
    "            \n",
    "            if update_info.batch.id % evaluate_every == 0:\n",
    "                yield(next(evaluate_updates))\n",
    "                \n",
    "    def updates(self, mode=\"train\"):\n",
    "        batches = self.batches(mode)\n",
    "        loss_sum = 0\n",
    "        \n",
    "        for batch in batches:\n",
    "            loss = self.batch_loss(batch) / (self.update_every * self.batch_size)\n",
    "            \n",
    "            loss.backward()\n",
    "            loss_sum += loss\n",
    "            \n",
    "            # we're doing the accumulated gradients trick to get the gradients variance\n",
    "            # down while being able to use commodity GPU:\n",
    "            if batch.id % self.update_every == 0:\n",
    "                if mode == \"train\":\n",
    "                    self.optimizer.step()\n",
    "                    self.optimizer.zero_grad()\n",
    "                \n",
    "                yield(UpdateInfo(batch, loss_sum, mode=mode))\n",
    "                \n",
    "                loss_sum = 0\n",
    "    \n",
    "    def test(self):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(BaseTrainer):\n",
    "    def __init__(*args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "    def compute_loss(self, word_embeddings, original_word_embeddings, discriminate_probs):\n",
    "        embeddings_loss = F.cosine_embedding_loss(\n",
    "            word_embeddings,\n",
    "            orig_word_embeddings,\n",
    "            torch.ones(word_embeddings.shape[0])\n",
    "        )\n",
    "        \n",
    "        discriminator_loss = F.binary_cross_entropy(\n",
    "            discriminate_probs,\n",
    "            torch.zeros_like(discriminate_probs)\n",
    "        )\n",
    "        \n",
    "        return embeddings_loss + discriminator_loss\n",
    "        \n",
    "\n",
    "    def batch_loss(self, batch):\n",
    "        word_embeddings, state = self.summarize_model(\n",
    "            batch.noisy_word_embeddings\n",
    "        )\n",
    "\n",
    "        # the discriminator guessing which mode (article or headline) was one state\n",
    "        # created for to deal with the \"segregation\" problem described in the paper:\n",
    "        discriminate_probs = self.discriminate_model(articles_state)\n",
    "\n",
    "        # we're diverging from the article here by outputting the word embeddings\n",
    "        # instead of the probabilities for each word in a vocabulary\n",
    "        # our loss function is using the cosine embedding loss coupled with\n",
    "        # the discriminator loss:\n",
    "        return self.compute_loss(\n",
    "            word_embeddings,\n",
    "            batch.word_embeddings,\n",
    "            discriminate_probs\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer('first-try', nlp, articles, 8)\n",
    "\n",
    "cumulative_train_info = UpdateInfo.empty()\n",
    "cumulative_evaluate_info = UpdateInfo.empty()\n",
    "\n",
    "for update_info in trainer.train_and_evaluate_updates():\n",
    "    if update_info.from_train:\n",
    "        cumulative_train_info += update_info\n",
    "        \n",
    "        print(f\"{cumulative_train_info}\")\n",
    "    \n",
    "    if update_info.from_evaluate:\n",
    "        cumulative_evaluate_info += update_info\n",
    "        \n",
    "        print(f\"{cumulative_evaluate_info}\")\n",
    "        \n",
    "        trainer.save_checkpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
