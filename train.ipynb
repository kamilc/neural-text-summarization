{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import functools\n",
    "import random\n",
    "import pdb\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import spacy\n",
    "from cached_property import cached_property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\n",
    "    \"en_core_web_lg\",\n",
    "    disable=[\"tagger\", \"ner\", \"textcat\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = pd.read_parquet(\"data/articles-processed.parquet.gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2 % 2 == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscriminatorNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DiscriminatorNet, self).__init__()\n",
    "\n",
    "    def forward(self, hidden_state):\n",
    "        \"\"\"\n",
    "        The forward pass for the network\n",
    "        \n",
    "        hidden_state : tensor (batch_num, hidden_size)\n",
    "        \n",
    "        returns         : tensor (batch_num, 1)\n",
    "        \"\"\"\n",
    "        \n",
    "        return hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SummarizeNet(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(SummarizeNet, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "    def forward(self, word_embeddings, generate=\"sentence\"):\n",
    "        \"\"\"\n",
    "        The forward pass for the network\n",
    "        \n",
    "        word_embeddings : tensor (batch_num, max_seq_len, vocab_len)\n",
    "        \n",
    "        returns         : tuple (\n",
    "                            tensor (batch_num, max_seq_len, vocab_len),\n",
    "                            tensor (batch_num, hidden_size)\n",
    "                          )\n",
    "        \n",
    "        First tensor in the returning tuple is a probability over the vocabulary\n",
    "        for each sequence position\n",
    "        \n",
    "        The second tensor is an encoder's hidden state \n",
    "        \"\"\"\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>text</th>\n",
       "      <th>normalized_title</th>\n",
       "      <th>set</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\nGet a bachelor’s degree.,\\nEnroll in a studi...</td>\n",
       "      <td>It is possible to become a VFX artist without...</td>\n",
       "      <td>HowtoBeaVisualEffectsArtist1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\nKeep your reference materials, sketches, art...</td>\n",
       "      <td>As you start planning for a project or work, ...</td>\n",
       "      <td>HowtoBeanOrganizedArtist2</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>\\nCreate a compelling reel or portfolio.,\\nLan...</td>\n",
       "      <td>This should be a short video showcasing the b...</td>\n",
       "      <td>HowtoBeaVisualEffectsArtist2</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>\\nJoin a professional society.,\\nEnjoy working...</td>\n",
       "      <td>Networking is a great way to find new opportu...</td>\n",
       "      <td>HowtoBeaVisualEffectsArtist3</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>\\nMake a list of what your friends watch, read...</td>\n",
       "      <td>Use your friends’ conversations to figure out...</td>\n",
       "      <td>HowtoAlwaysCatchPopCultureReferences1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215359</th>\n",
       "      <td>\\nUse a childhood nickname.,\\nUse your middle ...</td>\n",
       "      <td>You may have been called something other than...</td>\n",
       "      <td>HowtoPickaStageName2</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215360</th>\n",
       "      <td>\\nConsider changing the spelling of your name....</td>\n",
       "      <td>If you have a name that you like, you might f...</td>\n",
       "      <td>HowtoPickaStageName3</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215361</th>\n",
       "      <td>\\nTry out your name.,\\nDon’t legally change yo...</td>\n",
       "      <td>Your name might sound great to you when you s...</td>\n",
       "      <td>HowtoPickaStageName4</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215362</th>\n",
       "      <td>\\nUnderstand the process of relief printing.,\\...</td>\n",
       "      <td>Relief printing is the oldest and most tradit...</td>\n",
       "      <td>HowtoIdentifyPrints1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215363</th>\n",
       "      <td>\\nUnderstand the process of intaglio printing....</td>\n",
       "      <td>Intaglio is Italian for \"incis­ing,\" and corr...</td>\n",
       "      <td>HowtoIdentifyPrints2</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>130414 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 headline  \\\n",
       "index                                                       \n",
       "2       \\nGet a bachelor’s degree.,\\nEnroll in a studi...   \n",
       "4       \\nKeep your reference materials, sketches, art...   \n",
       "6       \\nCreate a compelling reel or portfolio.,\\nLan...   \n",
       "7       \\nJoin a professional society.,\\nEnjoy working...   \n",
       "9       \\nMake a list of what your friends watch, read...   \n",
       "...                                                   ...   \n",
       "215359  \\nUse a childhood nickname.,\\nUse your middle ...   \n",
       "215360  \\nConsider changing the spelling of your name....   \n",
       "215361  \\nTry out your name.,\\nDon’t legally change yo...   \n",
       "215362  \\nUnderstand the process of relief printing.,\\...   \n",
       "215363  \\nUnderstand the process of intaglio printing....   \n",
       "\n",
       "                                                     text  \\\n",
       "index                                                       \n",
       "2        It is possible to become a VFX artist without...   \n",
       "4        As you start planning for a project or work, ...   \n",
       "6        This should be a short video showcasing the b...   \n",
       "7        Networking is a great way to find new opportu...   \n",
       "9        Use your friends’ conversations to figure out...   \n",
       "...                                                   ...   \n",
       "215359   You may have been called something other than...   \n",
       "215360   If you have a name that you like, you might f...   \n",
       "215361   Your name might sound great to you when you s...   \n",
       "215362   Relief printing is the oldest and most tradit...   \n",
       "215363   Intaglio is Italian for \"incis­ing,\" and corr...   \n",
       "\n",
       "                             normalized_title    set  \n",
       "index                                                 \n",
       "2                HowtoBeaVisualEffectsArtist1  train  \n",
       "4                   HowtoBeanOrganizedArtist2  train  \n",
       "6                HowtoBeaVisualEffectsArtist2  train  \n",
       "7                HowtoBeaVisualEffectsArtist3  train  \n",
       "9       HowtoAlwaysCatchPopCultureReferences1  train  \n",
       "...                                       ...    ...  \n",
       "215359                   HowtoPickaStageName2  train  \n",
       "215360                   HowtoPickaStageName3  train  \n",
       "215361                   HowtoPickaStageName4  train  \n",
       "215362                   HowtoIdentifyPrints1  train  \n",
       "215363                   HowtoIdentifyPrints2  train  \n",
       "\n",
       "[130414 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArticlesDataset(Dataset):\n",
    "    def __init__(self, dataframe, mode, transforms=[]):\n",
    "        self.data = dataframe[dataframe.set == mode]\n",
    "        self.transforms = transforms\n",
    "        self.mode = mode\n",
    "        \n",
    "    def __len__(self):\n",
    "        return 2*len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        _idx = []\n",
    "        \n",
    "        if torch.is_tensor(idx):\n",
    "            _idx = idx.tolist()\n",
    "        \n",
    "        if isinstance(idx, list):\n",
    "            _idx = idx\n",
    "        else:\n",
    "            _idx = [ idx ]\n",
    "        \n",
    "        _ids = [ (i - (i % 2))/2 for i in _idx]\n",
    "\n",
    "        data = self.data.iloc[_ids, :]\n",
    "        data['asked_id'] = _idx\n",
    "        \n",
    "        data = pd.DataFrame(\n",
    "            {\n",
    "                'set': [self.mode for _ in range(0, len(_ids))],\n",
    "                'mode': [ (0 if i % 2 == 0 else 1) for i in _idx ],\n",
    "                'text': data.apply(lambda row: row['text'] if row['asked_id'] % 2 == 0 else row['headline'], axis=1),\n",
    "                'title': data['normalized_title']\n",
    "            }\n",
    "        )\n",
    "\n",
    "        for transform in self.transforms:\n",
    "            data = transform(data)\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextToParsedDoc(object):\n",
    "    def __init__(self, nlp):\n",
    "        self.nlp = nlp\n",
    "        \n",
    "    def __call__(self, sample):\n",
    "        sample['doc'] = sample.apply(lambda row: self.nlp(row['text']), axis=1)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordsToVectors(object):\n",
    "    def __init__(self, nlp):\n",
    "        self.nlp = nlp\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        \n",
    "        sample['word_embeddings'] = sample.apply(\n",
    "            lambda row: np.stack([token.vector for token in row['doc']]),\n",
    "            axis=1\n",
    "        )\n",
    "    \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddNoiseToEmbeddings(object):\n",
    "    def __init__(self, probability_of_mask_for_word):\n",
    "        self.probability_of_mask_for_word = probability_of_mask_for_word\n",
    "        self.rng = np.random.default_rng()\n",
    "        \n",
    "    def mask_vector(self, vector):\n",
    "        \"\"\"\n",
    "        Masks words with zeros randomly\n",
    "        \"\"\"\n",
    "        seq_len = vector.shape[0]\n",
    "        vector_len = vector.shape[1]\n",
    "        \n",
    "        mask = np.repeat(\n",
    "            self.rng.choice(\n",
    "                [0, 1],\n",
    "                seq_len,\n",
    "                p=[\n",
    "                    self.probability_of_mask_for_word,\n",
    "                    (1 - self.probability_of_mask_for_word)\n",
    "                ]\n",
    "            ).reshape((seq_len, 1)),\n",
    "            vector_len,\n",
    "            axis=1\n",
    "        )\n",
    "        \n",
    "        return vector * mask\n",
    "        \n",
    "    def __call__(self, sample):       \n",
    "        sample['noisy_word_embeddings'] = sample['word_embeddings'].apply(self.mask_vector)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MergeBatch(object):\n",
    "    def stack_vectors(self, vectors):\n",
    "        max_seq = max([vector.shape[0] for vector in vectors])\n",
    "        \n",
    "        return np.stack(\n",
    "            [\n",
    "                np.pad(vector, [(0, max_seq - vector.shape[0]), (0, 0)])\n",
    "                for vector in vectors\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "    def __call__(self, sample):\n",
    "        del sample['doc']\n",
    "        \n",
    "        sample = sample.to_dict(orient=\"list\")\n",
    "        \n",
    "        sample['word_embeddings'] = self.stack_vectors(sample['word_embeddings'])\n",
    "        sample['noisy_word_embeddings'] = self.stack_vectors(sample['noisy_word_embeddings'])\n",
    "    \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ArticlesDataset(\n",
    "    articles,\n",
    "    mode=\"train\",\n",
    "    transforms=[\n",
    "        TextToParsedDoc(nlp),\n",
    "        WordsToVectors(nlp),\n",
    "        AddNoiseToEmbeddings(0.2),\n",
    "        MergeBatch()\n",
    "    ]\n",
    ")[[0,1,2,3,4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArticlesBatch:\n",
    "    def __init__(self, data, id):\n",
    "        self.data = data\n",
    "        self.id = id\n",
    "        \n",
    "    def __getattr__(self, name):\n",
    "        if name in self.data:\n",
    "            return self.data[name]\n",
    "        else:\n",
    "            raise AttributeError(f\"Attribute missing: {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseTrainer:\n",
    "    def __init__(self, name, nlp, summarize_model, discriminate_model, dataframe,\n",
    "                 batch_size, update_every, save_every, loader_workers,\n",
    "                 probability_of_mask_for_word,\n",
    "                 lambda_article, lambda_sentence):\n",
    "        self.name = name\n",
    "        \n",
    "        self.datasets = {\n",
    "            \"train\": ArticlesDataset(\n",
    "                dataframe,\n",
    "                \"train\",\n",
    "                transforms=[\n",
    "                    TextToParsedDoc(nlp),\n",
    "                    WordsToVectors(nlp),\n",
    "                    AddNoiseToEmbeddings(\n",
    "                        probability_of_mask_for_word\n",
    "                    )\n",
    "                ]\n",
    "            ),\n",
    "            \"test\":  ArticlesDataset(\n",
    "                dataframe,\n",
    "                \"test\",\n",
    "                transforms=[\n",
    "                    TextToParsedDoc(nlp),\n",
    "                    WordsToVectors(nlp)\n",
    "                ]\n",
    "            ),\n",
    "            \"eval\":  ArticlesDataset(\n",
    "                dataframe,\n",
    "                \"eval\",\n",
    "                transforms=[\n",
    "                    TextToParsedDoc(nlp),\n",
    "                    WordsToVectors(nlp)\n",
    "                ]\n",
    "            )\n",
    "        }\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.update_every = update_every\n",
    "        self.save_every = save_every\n",
    "        self.loader_workers = loader_workers\n",
    "        \n",
    "        self.summarize_model = summarize_model\n",
    "        self.discriminate_model = discriminate_model\n",
    "        \n",
    "        self.lambda_article = lambda_article\n",
    "        self.lambda_sentence = lambda_sentence\n",
    "        \n",
    "        self.current_batch_id = 0\n",
    "        \n",
    "    @property\n",
    "    def models(self):\n",
    "        return self.summarize_model, self.discriminate_model\n",
    "    \n",
    "    def save(self):\n",
    "        checkpoint_path = f\"checkpoints/{self.name}/batch-#{self.current_batch_id}\"\n",
    "        os.makedirs(checkpoint_path, exist_ok=True)\n",
    "        \n",
    "        torch.save(\n",
    "            {\n",
    "                'current_batch_id': self.current_batch_id,\n",
    "                'batch_size': self.batch_size,\n",
    "                'update_every': self.update_every,\n",
    "                'save_every': self.save_every,\n",
    "                'lambda_article': self.lambda_article,\n",
    "                'lambda_sentence': self.lambda_sentence,\n",
    "                'summarize_model_state': self.summarize_model.state_dict(),\n",
    "                'discriminate_model_state': self.discriminate_model.state_dict(),\n",
    "                'optimizer_state_dict': self.optimizer.state_dict()\n",
    "            },\n",
    "            f\"{checkpoint_path}/state.pth\"\n",
    "        )\n",
    "        \n",
    "    def load(name, dataframe):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def batches(self, mode):\n",
    "        start_id = self.current_batch_id\n",
    "        \n",
    "        while True:\n",
    "            loader = DataLoader(\n",
    "                self.datasets[mode],\n",
    "                batch_size=self.batch_size,\n",
    "                shuffle=True,\n",
    "                num_workers=self.loader_workers\n",
    "            )\n",
    "\n",
    "            for ix, data in enumerate(loader):\n",
    "                self.current_batch_id += ix\n",
    "                \n",
    "                yield(\n",
    "                    ArticlesBatch(\n",
    "                        data,\n",
    "                        id=self.current_batch_id\n",
    "                    )\n",
    "                )\n",
    "        \n",
    "    \n",
    "    def batch_loss(self, batch):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def after_update(self, batch, loss_sum):\n",
    "        pass\n",
    "    \n",
    "    def train(self):\n",
    "        batches = self.batches(\"train\")\n",
    "        loss_sum = 0\n",
    "        \n",
    "        for batch in batches:\n",
    "            loss = self.batch_loss(batch) / (self.update_every * self.batch_size)\n",
    "            \n",
    "            loss.backward()\n",
    "            loss_sum += loss\n",
    "            \n",
    "            # we're doing the accumulated gradients trick to get the gradients variance\n",
    "            # down while being able to use commodity GPU:\n",
    "            if batch.id % self.update_every == 0:\n",
    "                self.optimizer.step()\n",
    "                self.optimizer.zero_grad()\n",
    "                \n",
    "                self.after_update(batch, loss_sum)\n",
    "                \n",
    "                loss_sum = 0\n",
    "    \n",
    "    def test(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def evaluate(self):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(BaseTrainer):\n",
    "    def __init__(self, name, nlp, summarize_model, discriminate_model, dataframe,\n",
    "                 batch_size, update_every, save_every, loader_workers,\n",
    "                 probability_of_mask_for_word, probability_of_masking_for_sample,\n",
    "                 lambda_article, lambda_sentence):\n",
    "        super().__init__(\n",
    "            name, nlp,\n",
    "            summarize_model, discriminate_model, dataframe,\n",
    "            batch_size, update_every, save_every, loader_workers,\n",
    "            lambda_article, lambda_sentence\n",
    "        )\n",
    "        \n",
    "    def compute_loss(self, articles_word_embeddings, orig_articles_word_embeddings,\n",
    "                     sentences_word_embeddings, orig_sentences_word_embeddings,\n",
    "                     sentences_numbers_in_articles,\n",
    "                     discriminate_articles_probs,\n",
    "                     discriminate_sentences_probs\n",
    "                    ):\n",
    "        articles_loss = F.cosine_embedding_loss(\n",
    "            articles_word_embeddings,\n",
    "            orig_articles_word_embeddings,\n",
    "            torch.ones(articles_word_embeddings.shape[0])\n",
    "        )\n",
    "        \n",
    "        sentences_loss = F.cosine_embedding_loss(\n",
    "            sentences_word_embeddings,\n",
    "            orig_sentences_word_embeddings,\n",
    "            torch.ones(articles_word_embeddings.shape[0])\n",
    "        ) / sentences_numbers_in_articles\n",
    "        \n",
    "        discriminator_articles_loss = F.binary_cross_entropy(\n",
    "            discriminate_articles_probs,\n",
    "            torch.zeros_like(discriminate_articles_probs)\n",
    "        )\n",
    "        \n",
    "        discriminator_sentences_loss = F.binary_cross_entropy(\n",
    "            discriminate_sentences_probs,\n",
    "            torch.zeros_like(discriminate_sentences_probs)\n",
    "        )\n",
    "        \n",
    "        return (articles_loss * self.lambda_article).sum(dim=0) +\n",
    "               (sentences_loss * self.lambda_sentence).sum(dim=0) +\n",
    "               discriminator_articles_loss +\n",
    "               discriminator_sentences_loss\n",
    "        \n",
    "\n",
    "    def batch_loss(self, batch):\n",
    "        # article -> article (de-noising)\n",
    "        articles_word_embeddings, articles_state = self.summarize_model(\n",
    "            batch.articles_noisy_word_embeddings,\n",
    "            generate=\"article\"\n",
    "        )\n",
    "\n",
    "        # headline -> headline (de-noising)\n",
    "        sentences_word_embeddings, sentences_state = self.summarize_model(\n",
    "            batch.sentences_noisy_word_embeddings,\n",
    "            generate=\"sentence\"\n",
    "        )\n",
    "\n",
    "        # the discriminator guessing which mode (article or sentence) was one state\n",
    "        # created for to deal with the \"segregation\" problem described in the paper:\n",
    "        discriminate_articles_probs = self.discriminate_model(articles_state)\n",
    "        discriminate_sentences_probs = self.discriminate_model(sentences_state)\n",
    "\n",
    "        # we're diverging from the article here by outputting the word embeddings\n",
    "        # instead of the probabilities for each word in a vocabulary\n",
    "        # our loss function is using the cosine embedding loss coupled with\n",
    "        # the discriminator loss:\n",
    "        return self.compute_loss(\n",
    "            articles_word_embeddings,\n",
    "            batch.articles_word_embeddings,\n",
    "\n",
    "            sentences_word_embeddings,\n",
    "            batch.sentences_word_embeddings,\n",
    "            batch.sentences_numbers_in_articles,\n",
    "\n",
    "            discriminate_articles_probs,\n",
    "            discriminate_sentences_probs\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(articles, 8)\n",
    "\n",
    "for epoch in trainer.train():\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
